# ìƒì„± ì‹œê°„: Sun Sep  7 21:21:09 KST 2025
# í•µì‹¬ ë‚´ìš©: ContentProcessingStage - í†µí•© ë¬¸ì„œ ì²˜ë¦¬ ë° ê°œì„ ëœ ëª©ì°¨ ìƒì„±
# ìƒì„¸ ë‚´ìš©:
#   - ContentProcessingStage (ë¼ì¸ 25-150): ë©”ì¸ ì»¨í…ì¸  ì²˜ë¦¬ í´ë˜ìŠ¤
#   - parse_unified_document (ë¼ì¸ 45-75): í†µí•© ë¬¸ì„œ íŒŒì‹±
#   - generate_extract_section (ë¼ì¸ 77-140): engines_v5.py íŒ¨í„´ í™œìš©í•œ 5ê°œ ì„¹ì…˜ ì¶”ì¶œ
#   - parse_extraction_response (ë¼ì¸ 142-180): AI ì‘ë‹µ 5ê°œ ì„¹ì…˜ìœ¼ë¡œ íŒŒì‹±
#   - load_and_sort_documents (ë¼ì¸ 182-250): ë¬¸ì„œ ë¡œë“œ ë° ë¦¬í”„/ë¹„ë¦¬í”„ ë¶„ë¦¬ ì •ë ¬
#   - format_extraction_content (ë¼ì¸ 252-270): ì¶”ì¶œ ê²°ê³¼ ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…
#   - update_extraction_section (ë¼ì¸ 272-320): íŒŒì¼ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸
# ìƒíƒœ: active

import os
import re
import glob
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# ì‹¤ì œ êµ¬í˜„ëœ ëª¨ë“ˆ í™œìš©
import sys
import os
# refactoring í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
refactoring_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, refactoring_root)
sys.path.append('/home/nadle/projects/Knowledge_Sherpa/v2/development/book_pipeline_refactored/src')

from src.utils.text_utils import normalize_title
from src.services.ai_service_v4 import AIService

# utils íŒŒì¼ ì‚­ì œë¡œ ì¸í•´ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ í´ë˜ìŠ¤ ë‚´ë¶€ì— êµ¬í˜„ (TEMP_IMPLì—ì„œ ê°€ì ¸ì˜´)

def combine_extraction_sections(extraction_result: Dict[str, str]) -> str:
    """ì¶”ì¶œ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    if not extraction_result:
        return ""
    
    formatted_parts = []
    
    # ì„¹ì…˜ ìˆœì„œëŒ€ë¡œ í¬ë§·íŒ…
    section_keys = ['core_content', 'detailed_core_content', 'detailed_content', 'main_topics', 'sub_topics']
    
    for key in section_keys:
        if key in extraction_result and extraction_result[key].strip():
            formatted_parts.append(extraction_result[key])
            formatted_parts.append("")  # ì„¹ì…˜ ê°„ ë¹ˆ ì¤„
    
    return "\n".join(formatted_parts)

def update_extraction_section(file_path: str, formatted_content: str) -> bool:
    """íŒŒì¼ì˜ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸"""
    if not formatted_content:
        print(f"âš ï¸ ì—…ë°ì´íŠ¸í•  ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {file_path}")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ì¶”ì¶œ ì„¹ì…˜ íŒ¨í„´ ì°¾ê¸°
        extraction_pattern = r'(# ì¶”ì¶œ\n---\n)(.*?)(?=\n# ë‚´ìš©|$)'
        
        if re.search(extraction_pattern, content, re.DOTALL):
            # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸
            new_content = re.sub(
                extraction_pattern,
                f'\\1{formatted_content}\n',
                content,
                flags=re.DOTALL
            )
        else:
            # ì¶”ì¶œ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ # ë‚´ìš© ì•ì— ì¶”ê°€
            content_pattern = r'(\n# ë‚´ìš©)'
            if re.search(content_pattern, content):
                new_content = re.sub(
                    content_pattern,
                    f'\n# ì¶”ì¶œ\n---\n{formatted_content}\n\\1',
                    content
                )
            else:
                # # ë‚´ìš© ì„¹ì…˜ë„ ì—†ìœ¼ë©´ ëì— ì¶”ê°€
                new_content = content + f'\n\n# ì¶”ì¶œ\n---\n{formatted_content}\n'
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"âœ… ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {file_path} - {e}")
        return False

def parse_extraction_response(response: str) -> Dict[str, str]:
    """ì¶”ì¶œ ì„¹ì…˜ íŒŒì‹± ìœ í‹¸ë¦¬í‹°"""
    sections = {}
    
    # ê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ
    patterns = {
        'core_content': r'## í•µì‹¬ ë‚´ìš©\n(.*?)(?=\n## |$)',
        'detailed_core_content': r'## ìƒì„¸ í•µì‹¬ ë‚´ìš©\n(.*?)(?=\n## |$)',
        'detailed_content': r'## ìƒì„¸ ì •ë³´\n(.*?)(?=\n## |$)',
        'main_topics': r'## ì£¼ìš” í™”ì œ\n(.*?)(?=\n## |$)',
        'sub_topics': r'## ë¶€ì°¨ í™”ì œ\n(.*?)(?=\n## |$)'
    }
    
    for section_key, pattern in patterns.items():
        match = re.search(pattern, response, re.DOTALL)
        if match:
            section_title = section_key.replace('_', ' ').replace('content', 'ë‚´ìš©').replace('detailed core', 'ìƒì„¸ í•µì‹¬').replace('detailed', 'ìƒì„¸ ì •ë³´').replace('main topics', 'ì£¼ìš” í™”ì œ').replace('sub topics', 'ë¶€ì°¨ í™”ì œ')
            sections[section_key] = f"## {section_title.title()}\n{match.group(1).strip()}"
    
    return sections

def update_file_process_status(file_path: str, status: bool) -> bool:
    """íŒŒì¼ì˜ process_status ì—…ë°ì´íŠ¸"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        status_value = 'true' if status else 'false'
        
        # ì†ì„± ì„¹ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
        attributes_pattern = r'(# ì†ì„±\n)(.*?)(?=\n# |$)'
        attributes_match = re.search(attributes_pattern, content, re.DOTALL)
        
        if attributes_match:
            # ê¸°ì¡´ ì†ì„± ì„¹ì…˜ ì—…ë°ì´íŠ¸
            attributes_content = attributes_match.group(2)
            
            # process_statusê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
            if 'process_status:' in attributes_content:
                # ê¸°ì¡´ process_status ê°’ ì—…ë°ì´íŠ¸
                updated_attributes = re.sub(
                    r'process_status:\s*(true|false)',
                    f'process_status: {status_value}',
                    attributes_content
                )
            else:
                # process_status ì¶”ê°€
                updated_attributes = attributes_content.strip() + f'\nprocess_status: {status_value}'
            
            new_content = re.sub(
                attributes_pattern,
                f'\\1{updated_attributes}\n',
                content,
                flags=re.DOTALL
            )
        else:
            # ì†ì„± ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì¶”ê°€ (# ë‚´ìš© ì•ì—)
            content_pattern = r'(\n# ë‚´ìš©)'
            if re.search(content_pattern, content):
                new_content = re.sub(
                    content_pattern,
                    f'\n# ì†ì„±\n---\nprocess_status: {status_value}\n\\1',
                    content
                )
            else:
                # # ë‚´ìš© ì„¹ì…˜ë„ ì—†ìœ¼ë©´ íŒŒì¼ ì‹œì‘ì— ì¶”ê°€
                new_content = f'# ì†ì„±\n---\nprocess_status: {status_value}\n\n{content}'
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"âœ… process_status ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_path} -> {status_value}")
        return True
        
    except Exception as e:
        print(f"âŒ process_status ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {file_path} - {e}")
        return False


class TaskTracker:
    """ë³‘ë ¬ ì²˜ë¦¬ íƒœìŠ¤í¬ ì¶”ì  í´ë˜ìŠ¤ - í…ŒìŠ¤íŠ¸ìš©"""
    
    def __init__(self):
        self.active_count = 0
        self.max_concurrent = 0
        self.task_start_times = {}
        self.task_end_times = {}
        self.task_counter = 0
    
    def task_start(self, task_id: str = None):
        """íƒœìŠ¤í¬ ì‹œì‘ ì¶”ì """
        if task_id is None:
            self.task_counter += 1
            task_id = f"task_{self.task_counter}"
        
        self.active_count += 1
        self.max_concurrent = max(self.max_concurrent, self.active_count)
        self.task_start_times[task_id] = asyncio.get_event_loop().time()
        return task_id
    
    def task_end(self, task_id: str):
        """íƒœìŠ¤í¬ ì¢…ë£Œ ì¶”ì """
        self.active_count -= 1
        self.task_end_times[task_id] = asyncio.get_event_loop().time()
    
    def get_stats(self):
        """ì¶”ì  í†µê³„ ë°˜í™˜"""
        return {
            'max_concurrent': self.max_concurrent,
            'total_tasks': self.task_counter,
            'completed_tasks': len(self.task_end_times),
            'task_durations': {
                task_id: self.task_end_times[task_id] - start_time 
                for task_id, start_time in self.task_start_times.items() 
                if task_id in self.task_end_times
            }
        }


class ContentProcessingStage:
    """ì»¨í…ì¸  ê°€ê³µ ë‹¨ê³„ - í†µí•© ë¬¸ì„œ ì²˜ë¦¬ ë° ê°œì„ ëœ ëª©ì°¨ ìƒì„±"""
    
    def __init__(self, config: Dict, ai_service: AIService):
        self.config = config
        self.ai_service = ai_service  
        self.processing_mode = config.get('processing_mode', 'unified_type_processing')
        self.max_parallel = config.get('max_parallel', 4)
        self.api_calls_counter = 0
        
        # config_manager ì´ˆê¸°í™”
        from src.utils.config_manager import ConfigManager
        self.config_manager = ConfigManager()
        
        # logger_v2 ì‚¬ìš©
        from src.utils.logger_v2 import Logger
        self.logger = Logger(self.__class__.__name__)
    
    def get_user_output_path(self) -> str:
        """Config Managerë¥¼ í†µí•´ ì‚¬ìš©ì ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        return self.config_manager.get(
            'content_processing.output_paths.user_output_path', 
            '/home/nadle/projects/Knowledge_Sherpa/v2/refactoring/tests/data'
        )

    async def parse_unified_document(self, file_path: str) -> Optional[Dict[str, Any]]:
        """ğŸ“„ í†µí•© ë¬¸ì„œ íŒŒì‹±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì œëª© ì¶”ì¶œ (íŒŒì¼ëª… ê¸°ë°˜)
            file_name = Path(file_path).name
            title_match = re.search(r'(\d+_lev\d+_.*?)_info\.md', file_name)
            title = title_match.group(1).replace('_', ' ') if title_match else file_name
            
            # level ì¶”ì¶œ 
            level_match = re.search(r'lev(\d+)', file_name)
            level = int(level_match.group(1)) if level_match else 0
            
            # êµ¬ì„± ì„¹ì…˜ ì¶”ì¶œ
            composition_match = re.search(r'# êµ¬ì„±\n(.*?)(?=\n# |$)', content, re.DOTALL)
            composition_section = composition_match.group(1).strip() if composition_match else '---'
            
            # ë‚´ìš© ì„¹ì…˜ ì¶”ì¶œ
            content_match = re.search(r'# ë‚´ìš©\n(.*?)(?=\n# |$)', content, re.DOTALL)
            content_section = content_match.group(1).strip() if content_match else ''
            
            # ì¶”ì¶œ ì„¹ì…˜ ì¶”ì¶œ (TOC ìƒì„± ì‹œ í•„ìš”)
            extraction_match = re.search(r'# ì¶”ì¶œ\n---\n(.*?)(?=\n# |$)', content, re.DOTALL)
            extraction_section = extraction_match.group(1).strip() if extraction_match else ''
            
            # ì†ì„± ì„¹ì…˜ì—ì„œ process_status ì¶”ì¶œ
            process_status = False
            attributes_match = re.search(r'# ì†ì„±\n(.*?)(?=\n# |$)', content, re.DOTALL)
            if attributes_match:
                attributes_content = attributes_match.group(1)
                if 'process_status: true' in attributes_content:
                    process_status = True
            
            return {
                'title': title,
                'level': level,
                'composition_section': composition_section,
                'content_section': content_section,
                'extraction_section': extraction_section,  # ì¶”ê°€
                'process_status': process_status,  # ì¶”ê°€
                'file_path': file_path,
                'full_content': content
            }
        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {file_path} - {e}")
            return None

    def _build_extraction_prompt(self, content: str, title: str) -> str:
        """ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ 5ê°€ì§€ ì •ë³´ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ì œëª©: {title}
ë¬¸ì„œ ë‚´ìš©:
{content}

ë‹¤ìŒ ìˆœì„œë¡œ ê° ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

## í•µì‹¬ ë‚´ìš©
ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½

## ìƒì„¸ í•µì‹¬ ë‚´ìš©
ì£¼ìš” ê°œë…ê³¼ ì¤‘ìš”í•œ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ 5-7ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬

## ìƒì„¸ ì •ë³´
ë¬¸ì„œì˜ ëª¨ë“  ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ì•Šê³  ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬

## ì£¼ìš” í™”ì œ
ë¬¸ì„œì—ì„œ ë‹¤ë£¨ëŠ” í•µì‹¬ ì£¼ì œë“¤ì„ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë‚˜ì—´

## ë¶€ì°¨ í™”ì œ
ì£¼ìš” ì£¼ì œ ì™¸ì— ì–¸ê¸‰ë˜ëŠ” ë¶€ì°¨ì ì¸ ì£¼ì œë“¤ì„ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë‚˜ì—´

**ì¤‘ìš” ê·œì¹™**: 
1. ê° ì„¹ì…˜ ì œëª©(## í•µì‹¬ ë‚´ìš©, ## ìƒì„¸ í•µì‹¬ ë‚´ìš© ë“±)ì„ í•œ ë²ˆë§Œ ì¶œë ¥í•˜ê³  ë°”ë¡œ ë‹¤ìŒ ì¤„ì— ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.
2. ë¹ˆ í—¤ë” ë¼ì¸ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
3. ì„¹ì…˜ ë‚´ìš©ì„ ì‘ì„±í•  ë•Œ í—¤ë”ê°€ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ë°˜ë“œì‹œ ### (í•´ì‹œ 3ê°œ) ì´ìƒì˜ í—¤ë”ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
4. ## í—¤ë”ëŠ” ì„¹ì…˜ ì œëª©ê³¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì ˆëŒ€ ì¤‘ë³µ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

    def _get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return """ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€. ì£¼ì–´ì§„ 5ê°€ì§€ ì •ë³´ íƒ€ì…ì„ ìˆœì„œëŒ€ë¡œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”.
- í•µì‹¬ ë‚´ìš©: ê°„ê²°í•˜ê³  ì •í™•í•œ ìš”ì•½
- ìƒì„¸ í•µì‹¬ ë‚´ìš©: ìƒì„¸í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ë‚´ìš©
- ìƒì„¸ ì •ë³´: ì²´ê³„ì ì´ê³  í¬ê´„ì ì¸ ì •ë¦¬
- ì£¼ìš” í™”ì œ: í•µì‹¬ ì£¼ì œë“¤
- ë¶€ì°¨ í™”ì œ: ë¶€ì°¨ì ì´ì§€ë§Œ ì˜ë¯¸ìˆëŠ” ì£¼ì œë“¤

ì •í™•í•œ í˜•ì‹ì„ ì§€ì¼œì„œ ì¶œë ¥í•˜ì„¸ìš”."""

    def _validate_extraction_sections(self, sections: Dict[str, str], title: str) -> bool:
        """ì¶”ì¶œëœ ì„¹ì…˜ë“¤ì˜ ìœ íš¨ì„± ê²€ì¦"""
        success_count = sum(1 for content in sections.values() 
                           if content.strip() and content.startswith('##'))
        
        if success_count >= 3:  # engines_v5.pyì™€ ë™ì¼í•œ ê¸°ì¤€
            self.api_calls_counter += 1
            self.logger.info(f"âœ… ì¶”ì¶œ ì„±ê³µ: {title} ({success_count}/5 ì„¹ì…˜)")
            return True
        else:
            self.logger.warning(f"âš ï¸ ì¶”ì¶œ ì„¹ì…˜ ë¶ˆì™„ì „: {title} ({success_count}/5)")
            return False

    async def get_combined_content(self, doc: Dict) -> str:
        """ğŸ“– ë…¸ë“œì˜ í†µí•© ì½˜í…ì¸  ìƒì„± (ë¦¬í”„: ìì‹ ë§Œ, ë¹„ë¦¬í”„: ìì‹ +êµ¬ì„±ë…¸ë“œë“¤)"""
        base_content = doc.get('content_section', '').strip()
        composition_files = doc.get('composition_files', [])
        
        # ë¦¬í”„ ë…¸ë“œì¸ ê²½ìš° ìì‹ ì˜ ì½˜í…ì¸ ë§Œ ë°˜í™˜
        if not composition_files:
            return base_content
        
        # ë¹„ë¦¬í”„ ë…¸ë“œì¸ ê²½ìš°: ìì‹ ì˜ ë‚´ìš© + êµ¬ì„± ë…¸ë“œë“¤ì˜ ë‚´ìš© ê²°í•©
        combined_content = base_content if base_content else ""
        
        self.logger.info(f"ğŸ”— ë¹„ë¦¬í”„ ë…¸ë“œ êµ¬ì„± íŒŒì¼ ê²°í•©: {len(composition_files)}ê°œ íŒŒì¼")
        
        # docì— ì´ë¯¸ êµ¬ì„± ë…¸ë“œë“¤ì˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        # composition_sectionì„ íŒŒì‹±í•´ì„œ ê° êµ¬ì„± íŒŒì¼ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
        composition_section = doc.get('composition_section', '')
        
        if composition_section and composition_section.strip() != '---':
            # composition_sectionì—ì„œ êµ¬ì„± íŒŒì¼ë“¤ íŒŒì‹±
            for comp_file in composition_files:
                # TODO: ì—¬ê¸°ì„œ ì‹¤ì œ êµ¬ì„± íŒŒì¼ ì½ê¸° ë˜ëŠ” full_contentì—ì„œ ì¶”ì¶œ
                # í˜„ì¬ëŠ” íŒŒì¼ëª…ë§Œ í‘œì‹œ
                combined_content += f"\n\n### êµ¬ì„± íŒŒì¼: {comp_file}"
                self.logger.debug(f"  ğŸ“„ êµ¬ì„± íŒŒì¼ ì¶”ê°€: {comp_file}")
        
        return combined_content

    async def generate_extract_section(self, doc: Dict) -> Dict[str, str]:
        """ğŸ¤– engines_v5.py íŒ¨í„´ í™œìš©í•œ 5ê°œ ì„¹ì…˜ ì¶”ì¶œ"""
        title = doc.get('title', '')
        
        # í†µí•© ì½˜í…ì¸  ìƒì„± (ë¦¬í”„: ìì‹ ë§Œ, ë¹„ë¦¬í”„: ìì‹ +êµ¬ì„±ë…¸ë“œë“¤)
        content = await self.get_combined_content(doc)
        
        if not content.strip():
            self.logger.warning(f"âš ï¸ í†µí•© ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŒ: {title}")
            return {}
        
        try:
            self.logger.info(f"ğŸ¤– AI ì¶”ì¶œ ì‹œì‘: {title}")
            
            # í•¨ìˆ˜ë¡œ ë¶„ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_extraction_prompt(content, title)
            system_prompt = self._get_system_prompt()
            
            # AI ì„œë¹„ìŠ¤ í˜¸ì¶œ (content_processing ì„¤ì • í™œìš©)
            response = await self.ai_service.query_single_request(
                prompt=prompt,
                additional_data={'system_prompt': system_prompt}
            )
            
            # engines_v5.py íŒŒì‹± ë¡œì§ í™œìš© (utils í•¨ìˆ˜ ì‚¬ìš©)
            sections = parse_extraction_response(response)
            
            # í•¨ìˆ˜ë¡œ ë¶„ë¦¬ëœ ê²€ì¦ ë¡œì§
            if self._validate_extraction_sections(sections, title):
                return sections
            else:
                return {}
                
        except Exception as e:
            self.logger.error(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {title} - {e}")
            return {}


    async def load_and_sort_documents(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ“š í†µí•© ë¬¸ì„œ ë¡œë“œ ë° ì¥ë³„ ê·¸ë£¹í™” - ë¦¬í”„/ë¹„ë¦¬í”„ ë¶„ë¦¬"""
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            processed_chapters, unified_documents = self._extract_input_data(input_data)
            if not processed_chapters or not unified_documents:
                return self._create_empty_result()
            
            self.logger.info(f"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ: {len(unified_documents)}ê°œ, ì¥ ìˆ˜: {len(processed_chapters)}")
            
            # ì¥ë³„ ì²˜ë¦¬
            chapters_result = await self._process_chapters(processed_chapters, unified_documents)
            
            self.logger.info(f"ğŸ“‹ ì¥ë³„ ê·¸ë£¹í™” ì™„ë£Œ: {len(chapters_result)}ê°œ ì¥")
            
            return {
                "output": {"chapters": chapters_result},
                "error": None
            }
            
        except Exception as e:
            return self._create_error_result(e)
    
    def _extract_input_data(self, input_data: Dict[str, Any]) -> tuple:
        """ì…ë ¥ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ"""
        processed_chapters = input_data.get('processed_chapters', [])
        unified_documents = input_data.get('unified_documents', [])
        return processed_chapters, unified_documents
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        self.logger.warning("âš ï¸ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {
            "output": {"chapters": []},
            "error": None
        }
    
    def _create_error_result(self, error: Exception) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        error_msg = f"load_and_sort_documents ì‹¤í–‰ ì˜¤ë¥˜: {str(error)}"
        self.logger.error(f"âŒ {error_msg}")
        return {
            "output": {"chapters": []},
            "error": error_msg
        }
    
    async def _process_chapters(self, processed_chapters: List[Dict], unified_documents: List[Dict]) -> List[Dict]:
        """ì¥ë³„ ë¬¸ì„œ ì²˜ë¦¬"""
        chapters_result = []
        
        for chapter_index, chapter_info in enumerate(processed_chapters):
            chapter_result = await self._process_single_chapter(
                chapter_index, chapter_info, unified_documents
            )
            if chapter_result:
                chapters_result.append(chapter_result)
        
        return chapters_result
    
    async def _process_single_chapter(self, chapter_index: int, chapter_info: Dict, unified_documents: List[Dict]) -> Optional[Dict]:
        """ë‹¨ì¼ ì¥ ì²˜ë¦¬"""
        normalized_title = chapter_info.get('normalized_title', '')
        chapter_title = chapter_info.get('chapter_title', '')
        
        # í•´ë‹¹ ì¥ì˜ ë¬¸ì„œë“¤ ì°¾ê¸°
        chapter_documents = await self._find_chapter_documents(
            normalized_title, chapter_title, unified_documents
        )
        
        if not chapter_documents:
            return None
        
        # ë¦¬í”„/ë¹„ë¦¬í”„ ë¶„ë¦¬
        leaf_nodes, non_leaf_nodes = self._separate_leaf_and_non_leaf(chapter_documents)
        
        # ê²°ê³¼ êµ¬ì„±
        return {
            "chapter_title": normalized_title,  # normalized_title ì‚¬ìš©
            "leaf_nodes": leaf_nodes,
            "non_leaf_nodes": non_leaf_nodes
        }
    
    async def _find_chapter_documents(self, normalized_title: str, chapter_title: str, unified_documents: List[Dict]) -> List[Dict]:
        """íŠ¹ì • ì¥ì˜ ë¬¸ì„œë“¤ ì°¾ê¸° ë° íŒŒì‹±"""
        chapter_documents = []
        
        for doc in unified_documents:
            file_name = doc.get('file_name', '')
            if normalized_title in file_name:
                parsed_doc = await self.parse_unified_document_from_content(doc.get('content', ''), file_name)
                if parsed_doc:
                    # ì¥ ì •ë³´ ì¶”ê°€
                    parsed_doc['chapter_info'] = {
                        'chapter_title': chapter_title,
                        'normalized_title': normalized_title
                    }
                    chapter_documents.append(parsed_doc)
        
        return chapter_documents
    
    def _separate_leaf_and_non_leaf(self, chapter_documents: List[Dict]) -> tuple:
        """ë¦¬í”„ ë…¸ë“œì™€ ë¹„ë¦¬í”„ ë…¸ë“œ ë¶„ë¦¬ - levelë³„ ê·¸ë£¹í™”"""
        leaf_nodes = []
        non_leaf_groups = {}
        
        for doc in chapter_documents:
            composition_files = doc.get('composition_files', [])
            if not composition_files:  # ë¹ˆ ë°°ì—´ = ë¦¬í”„ ë…¸ë“œ
                leaf_nodes.append(doc)
            else:  # ë°°ì—´ì— ìš”ì†Œ ìˆìŒ = ë¹„ë¦¬í”„ ë…¸ë“œ
                level = doc.get('level', 0)
                level_key = f"level_{level}"
                
                if level_key not in non_leaf_groups:
                    non_leaf_groups[level_key] = []
                non_leaf_groups[level_key].append(doc)
        
        # level ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        sorted_non_leaf_groups = {}
        for level in sorted(non_leaf_groups.keys(), key=lambda x: int(x.split('_')[1]), reverse=True):
            sorted_non_leaf_groups[level] = non_leaf_groups[level]
        
        return leaf_nodes, sorted_non_leaf_groups
        
        self.logger.info(f"ğŸ“‹ ì¥ë³„ ê·¸ë£¹í™” ì™„ë£Œ: {len(chapter_groups)}ê°œ ì¥")
        return chapter_groups
    
    async def parse_unified_document_from_content(self, content: str, file_name: str) -> Optional[Dict[str, Any]]:
        """ğŸ“„ ë©”ëª¨ë¦¬ìƒ í†µí•© ë¬¸ì„œ contentì—ì„œ íŒŒì‹±"""
        try:
            # ì œëª© ì¶”ì¶œ (íŒŒì¼ëª… ê¸°ë°˜)
            title_match = re.search(r'(\d+_lev\d+_.*?)_info\.md', file_name)
            title = title_match.group(1).replace('_', ' ') if title_match else file_name
            
            # level ì¶”ì¶œ 
            level_match = re.search(r'lev(\d+)', file_name)
            level = int(level_match.group(1)) if level_match else 0
            
            # êµ¬ì„± ì„¹ì…˜ ì¶”ì¶œ
            composition_match = re.search(r'# êµ¬ì„±\n(.*?)(?=\n# |$)', content, re.DOTALL)
            composition_section = composition_match.group(1).strip() if composition_match else '---'
            
            # ë‚´ìš© ì„¹ì…˜ ì¶”ì¶œ
            content_match = re.search(r'# ë‚´ìš©\n(.*?)(?=\n# |$)', content, re.DOTALL)
            content_section = content_match.group(1).strip() if content_match else ''
            
            # ì¶”ì¶œ ì„¹ì…˜ ì¶”ì¶œ
            extraction_match = re.search(r'# ì¶”ì¶œ\n---\n(.*?)(?=\n# |$)', content, re.DOTALL)
            extraction_section = extraction_match.group(1).strip() if extraction_match else ''
            
            # ì†ì„± ì„¹ì…˜ì—ì„œ process_status ì¶”ì¶œ
            process_status = False
            attributes_match = re.search(r'# ì†ì„±\n(.*?)(?=\n# |$)', content, re.DOTALL)
            if attributes_match:
                attributes_content = attributes_match.group(1)
                if 'process_status: true' in attributes_content:
                    process_status = True
            
            # composition_files ìƒì„± (composition_sectionì´ "---"ê°€ ì•„ë‹ˆë©´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
            composition_files = []
            if composition_section != "---" and composition_section.strip():
                # êµ¬ì„± ì„¹ì…˜ì—ì„œ íŒŒì¼ëª…ë“¤ ì¶”ì¶œ
                lines = composition_section.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if line and line.endswith('.md'):
                        composition_files.append(line)
            
            return {
                'title': title,
                'level': level,
                'composition_section': composition_section,
                'content_section': content_section,
                'extraction_section': extraction_section,
                'process_status': process_status,
                'file_name': file_name,
                'full_content': content,
                'composition_files': composition_files
            }
        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {file_name} - {e}")
            return None
    
    def sort_documents_by_level(self, documents: List[Dict]) -> List[List[Dict]]:
        """ğŸ“Š ë¬¸ì„œë“¤ì„ ë¦¬í”„/ë¹„ë¦¬í”„ ë¶„ë¦¬ í›„ levelë³„ ì •ë ¬"""
        # ë¦¬í”„/ë¹„ë¦¬í”„ ë¶„ë¦¬
        leaf_nodes = []
        non_leaf_nodes = []
        
        for doc in documents:
            composition_section = doc.get('composition_section', '').strip()
            if composition_section and composition_section != '---':
                # êµ¬ì„± ë…¸ë“œ íŒŒì¼ëª…ë“¤ì´ ìˆëŠ” ê²½ìš° (ë¹„ë¦¬í”„)
                composition_lines = [line.strip() for line in composition_section.split('\n') 
                                   if line.strip() and not line.startswith('---')]
                doc['composition_files'] = composition_lines
                non_leaf_nodes.append(doc)
            else:
                # êµ¬ì„± ì„¹ì…˜ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° (ë¦¬í”„)
                doc['composition_files'] = []
                leaf_nodes.append(doc)
        
        # levelë³„ ê·¸ë£¹í™” (ë¹„ë¦¬í”„ ë…¸ë“œë“¤)
        level_groups = {}
        for doc in non_leaf_nodes:
            level = doc.get('level', 0)
            if level not in level_groups:
                level_groups[level] = []
            level_groups[level].append(doc)
        
        # ìµœì¢… ì •ë ¬ëœ ê·¸ë£¹ë“¤ (ë¦¬í”„ ë…¸ë“œê°€ ë¨¼ì €, ê·¸ë‹¤ìŒ level ë‚´ë¦¼ì°¨ìˆœ)
        sorted_groups = []
        if leaf_nodes:
            sorted_groups.append(leaf_nodes)  # ë¦¬í”„ ë…¸ë“œ ê·¸ë£¹ì´ ë¨¼ì €
        
        for level in sorted(level_groups.keys(), reverse=True):  # level ë‚´ë¦¼ì°¨ìˆœ
            sorted_groups.append(level_groups[level])
        
        return sorted_groups



    async def add_update_status_mark(self, file_path: str, mark: str):
        """ğŸ·ï¸ íŒŒì¼ì— ìƒíƒœ ë§ˆí‚¹ ì¶”ê°€ - # ì¶”ì¶œ --- ë°”ë¡œ ë‹¤ìŒ ì¤„ì— ìœ„ì¹˜"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # # ì¶”ì¶œ --- ë°”ë¡œ ë‹¤ìŒì— ë§ˆí‚¹ ì¶”ê°€ (í•µì‹¬ë‚´ìš© ë°”ë¡œ ìœ„)
            extraction_pattern = r'(# ì¶”ì¶œ\n---\n)(.*?)(\n# ë‚´ìš©|$)'
            
            if re.search(extraction_pattern, content, re.DOTALL):
                new_content = re.sub(
                    extraction_pattern,
                    f'\\1{mark}\n\n\\2\\3',
                    content,
                    flags=re.DOTALL
                )
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                    
                self.logger.info(f"ğŸ·ï¸ ìƒíƒœ ë§ˆí‚¹ ì¶”ê°€: {Path(file_path).name} - {mark}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìƒíƒœ ë§ˆí‚¹ ì‹¤íŒ¨: {file_path} - {e}")

    async def process_group_sequential(self, group: List[Dict], user_output_path: str) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ë‚´ ìˆœì°¨ ì²˜ë¦¬ - ë‹¨ì¼ ê·¸ë£¹ì˜ ë¬¸ì„œë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
        
        Args:
            group: ì²˜ë¦¬í•  ë¬¸ì„œ ê·¸ë£¹ (ë¦¬ìŠ¤íŠ¸)
            user_output_path: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ê²½ë¡œ
            
        Returns:
            {"output": "success", "error": None} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info(f"ğŸ”„ ê·¸ë£¹ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘: {len(group)}ê°œ ë¬¸ì„œ")
            
            processed_count = 0
            for doc in group:
                try:
                    # process_single_document í˜¸ì¶œ
                    result = await self.process_single_document(doc, user_output_path)
                    
                    if result.get('error') is None:
                        processed_count += 1
                        self.logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {doc.get('title', 'Unknown')}")
                    else:
                        self.logger.warning(f"âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {doc.get('title', 'Unknown')} - {result.get('error')}")
                        
                except Exception as e:
                    self.logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {doc.get('title', 'Unknown')} - {e}")
                    continue
            
            self.logger.info(f"âœ… ê·¸ë£¹ ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}/{len(group)}ê°œ ì„±ê³µ")
            
            return {
                "output": f"success: {processed_count}/{len(group)} documents processed",
                "error": None
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¸ë£¹ ìˆœì°¨ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "output": None,
                "error": str(e)
            }

    async def process_group_parallel(self, group: List[Dict], user_output_path: str) -> Dict[str, Any]:
        """
        ê·¸ë£¹ ë‚´ ë³‘ë ¬ ì²˜ë¦¬ - ë‹¨ì¼ ê·¸ë£¹ì˜ ë¬¸ì„œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        
        Args:
            group: ì²˜ë¦¬í•  ë¬¸ì„œ ê·¸ë£¹ (ë¦¬ìŠ¤íŠ¸)
            user_output_path: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ê²½ë¡œ
            
        Returns:
            {"output": "success: X/Y documents processed", "error": None} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info(f"ğŸ”„ ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(group)}ê°œ ë¬¸ì„œ")
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ (max_parallel ê¸°ë³¸ê°’: 4)
            semaphore = asyncio.Semaphore(self.max_parallel)
            
            async def process_single_doc(doc):
                async with semaphore:
                    return await self.process_single_document(doc, user_output_path)
            
            # ëª¨ë“  ë¬¸ì„œ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [process_single_doc(doc) for doc in group]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì§‘ê³„
            processed_count = 0
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {group[i].get('title', 'Unknown')} - {result}")
                    continue
                elif result and result.get('error') is None:
                    processed_count += 1
                    self.logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {group[i].get('title', 'Unknown')}")
                else:
                    self.logger.warning(f"âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {group[i].get('title', 'Unknown')} - {result.get('error') if result else 'Unknown error'}")
            
            self.logger.info(f"âœ… ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}/{len(group)}ê°œ ì„±ê³µ")
            
            return {
                "output": f"success: {processed_count}/{len(group)} documents processed",
                "error": None
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "output": None,
                "error": str(e)
            }

    async def process_group_parallel_with_tracking(self, group: List[Dict], user_output_path: str) -> tuple[Dict[str, Any], TaskTracker]:
        """
        í…ŒìŠ¤íŠ¸ìš© - íƒœìŠ¤í¬ ì¶”ì  ê¸°ëŠ¥ì´ ìˆëŠ” ë³‘ë ¬ ì²˜ë¦¬
        
        Args:
            group: ì²˜ë¦¬í•  ë¬¸ì„œ ê·¸ë£¹ (ë¦¬ìŠ¤íŠ¸)
            user_output_path: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ê²½ë¡œ
            
        Returns:
            (result, tracker): ì²˜ë¦¬ ê²°ê³¼ì™€ íƒœìŠ¤í¬ ì¶”ì  ì •ë³´
        """
        tracker = TaskTracker()
        
        try:
            self.logger.info(f"ğŸ”„ ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì¶”ì  ëª¨ë“œ): {len(group)}ê°œ ë¬¸ì„œ")
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ (max_parallel ê¸°ë³¸ê°’: 4)
            semaphore = asyncio.Semaphore(self.max_parallel)
            
            async def process_single_doc_tracked(doc):
                task_id = tracker.task_start(f"doc_{doc.get('title', 'unknown')}")
                async with semaphore:
                    try:
                        # í˜„ì¬ í™œì„± íƒœìŠ¤í¬ ìˆ˜ ë¡œê¹…
                        self.logger.info(f"ğŸ”„ í™œì„± íƒœìŠ¤í¬ ìˆ˜: {tracker.active_count} (ìµœëŒ€: {tracker.max_concurrent})")
                        result = await self.process_single_document(doc, user_output_path)
                        return result
                    finally:
                        tracker.task_end(task_id)
            
            # ëª¨ë“  ë¬¸ì„œ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [process_single_doc_tracked(doc) for doc in group]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì§‘ê³„
            processed_count = 0
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {group[i].get('title', 'Unknown')} - {result}")
                    continue
                elif result and result.get('error') is None:
                    processed_count += 1
                    self.logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {group[i].get('title', 'Unknown')}")
                else:
                    self.logger.warning(f"âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {group[i].get('title', 'Unknown')} - {result.get('error') if result else 'Unknown error'}")
            
            # ì¶”ì  í†µê³„ ë¡œê¹…
            stats = tracker.get_stats()
            self.logger.info(f"ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ í†µê³„: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ {stats['max_concurrent']}ê°œ, ì™„ë£Œ {stats['completed_tasks']}/{stats['total_tasks']}ê°œ")
            
            self.logger.info(f"âœ… ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}/{len(group)}ê°œ ì„±ê³µ")
            
            result = {
                "output": f"success: {processed_count}/{len(group)} documents processed",
                "error": None
            }
            
            return result, tracker
            
        except Exception as e:
            self.logger.error(f"âŒ ê·¸ë£¹ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            result = {
                "output": None,
                "error": str(e)
            }
            return result, tracker

    async def process_document_groups(self, sorted_data: Dict, user_output_path: str, parallel: bool = False) -> Dict[str, Any]:
        """
        ì±•í„°ë³„ ê·¸ë£¹ ì²˜ë¦¬ - ê° ì±•í„°ë§ˆë‹¤ [ë¦¬í”„] -> [ë ˆë²¨3] -> [ë ˆë²¨2] -> [ë ˆë²¨1] ìˆœì„œ
        
        Args:
            sorted_data: load_and_sort_documents ê²°ê³¼ ë°ì´í„°
            user_output_path: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ê²½ë¡œ
            parallel: Trueë©´ ê·¸ë£¹ ë‚´ ë³‘ë ¬ ì²˜ë¦¬, Falseë©´ ìˆœì°¨ ì²˜ë¦¬ (ê¸°ë³¸ê°’)
            
        Returns:
            {"output": "success: X documents processed in Y groups across Z chapters", "error": None} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        try:
            chapters_data = sorted_data.get('output', {}).get('chapters', [])
            
            if not chapters_data:
                self.logger.warning("âš ï¸ ì²˜ë¦¬í•  ì¥ì´ ì—†ìŠµë‹ˆë‹¤")
                return {"output": "success: no chapters to process", "error": None}
            
            total_processed = 0
            total_groups = 0
            
            # ì¥ë³„ ìˆœì°¨ ì²˜ë¦¬
            for chapter_idx, chapter in enumerate(chapters_data):
                self.logger.info(f"ğŸ“š ì œ{chapter_idx + 1}ì¥ ì²˜ë¦¬ ì‹œì‘")
                
                # 1. ë¦¬í”„ ë…¸ë“œ ê·¸ë£¹ ì²˜ë¦¬ (ìµœìš°ì„ )
                leaf_nodes = chapter.get('leaf_nodes', [])
                if leaf_nodes:
                    self.logger.info(f"  ğŸƒ ë¦¬í”„ë…¸ë“œ ê·¸ë£¹: {len(leaf_nodes)}ê°œ ë¬¸ì„œ ì²˜ë¦¬")
                    if parallel:
                        result = await self.process_group_parallel(leaf_nodes, user_output_path)
                    else:
                        result = await self.process_group_sequential(leaf_nodes, user_output_path)
                    total_groups += 1
                    if result.get('error') is None:
                        # ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜ ì¶”ì¶œ (ì˜ˆ: "success: 3/3 documents processed")
                        output_str = result.get('output', '')
                        if 'success:' in output_str and 'documents processed' in output_str:
                            processed_part = output_str.split('success:')[1].split('documents processed')[0].strip()
                            if '/' in processed_part:
                                processed_count = int(processed_part.split('/')[0])
                                total_processed += processed_count
                    self.logger.info(f"  âœ… ë¦¬í”„ë…¸ë“œ ì²˜ë¦¬ ì™„ë£Œ")
                
                # 2. ë¹„ë¦¬í”„ ë…¸ë“œ - ë ˆë²¨ ë‚´ë¦¼ì°¨ìˆœ ì²˜ë¦¬
                non_leaf_nodes = chapter.get('non_leaf_nodes', {})
                
                # level_3 -> level_2 -> level_1 ìˆœì„œë¡œ ì²˜ë¦¬
                for level_key in sorted(non_leaf_nodes.keys(), 
                                      key=lambda x: int(x.split('_')[1]), 
                                      reverse=True):
                    nodes = non_leaf_nodes[level_key]
                    if nodes:
                        level_num = level_key.split('_')[1]
                        self.logger.info(f"  ğŸ”¢ ë ˆë²¨ {level_num} ê·¸ë£¹: {len(nodes)}ê°œ ë¬¸ì„œ ì²˜ë¦¬")
                        if parallel:
                            result = await self.process_group_parallel(nodes, user_output_path)
                        else:
                            result = await self.process_group_sequential(nodes, user_output_path)
                        total_groups += 1
                        if result.get('error') is None:
                            # ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜ ì¶”ì¶œ
                            output_str = result.get('output', '')
                            if 'success:' in output_str and 'documents processed' in output_str:
                                processed_part = output_str.split('success:')[1].split('documents processed')[0].strip()
                                if '/' in processed_part:
                                    processed_count = int(processed_part.split('/')[0])
                                    total_processed += processed_count
                        self.logger.info(f"  âœ… ë ˆë²¨ {level_num} ì²˜ë¦¬ ì™„ë£Œ")
                
                self.logger.info(f"ğŸ¯ ì œ{chapter_idx + 1}ì¥ ì²˜ë¦¬ ì™„ë£Œ")
            
            self.logger.info(f"ğŸ† ì „ì²´ {len(chapters_data)}ê°œ ì¥, {total_groups}ê°œ ê·¸ë£¹ ì²˜ë¦¬ ì™„ë£Œ! (ì´ {total_processed}ê°œ ë¬¸ì„œ)")
            
            return {
                "output": f"success: {total_processed} documents processed in {total_groups} groups across {len(chapters_data)} chapters",
                "error": None
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì±•í„°ë³„ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "output": None,
                "error": str(e)
            }
    
    def _extract_chapter_files_from_prev_result(self, prev_stage_result: Dict[str, Any]) -> Dict[str, Any]:
        """prev_stage_resultì—ì„œ ì¥ë³„ë¡œ íŒŒì¼ëª… ê·¸ë£¹í™”í•´ì„œ ë°˜í™˜"""
        chapter_files = {}
        
        data = prev_stage_result.get('data', {})
        processed_chapters = data.get('processed_chapters', [])
        unified_documents = data.get('unified_documents', [])
        
        # ê° ì¥ë³„ë¡œ ì²˜ë¦¬
        for chapter in processed_chapters:
            normalized_title = chapter.get('normalized_title', '')
            chapter_title = chapter.get('chapter_title', 'Unknown')
            
            # í•´ë‹¹ ì¥ì˜ ë¬¸ì„œë“¤ ì°¾ê¸° (normalized_titleë¡œ ë§¤ì¹­)
            file_names = []
            for doc in unified_documents:
                file_name = doc.get('file_name', '')
                if normalized_title in file_name:
                    file_names.append(file_name)
            
            # chapter_titleì„ í‚¤ë¡œ ì‚¬ìš©
            chapter_files[chapter_title] = file_names
        
        return chapter_files

    async def process(self, prev_stage_result: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸš€ ë©”ì¸ ì²˜ë¦¬ ë¡œì§ - prev_stage_result ê¸°ë°˜"""
        try:
            self.logger.info("ğŸš€ ContentProcessingStage ì‹œì‘")
            
            # 1. ê°€ë…ì„±ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ì¶œ
            unified_documents = prev_stage_result['data']
            
            # 2. ë¬¸ì„œ ë¡œë“œ ë° ì •ë ¬ 
            sorted_data = await self.load_and_sort_documents(unified_documents)
            
            if not sorted_data or not sorted_data.get('output', {}).get('chapters'):
                return {'data': {}, 'error': 'ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # 3. config_manager ê¸°ë°˜ ê²½ë¡œ ì‚¬ìš©
            result = await self.process_document_groups(sorted_data, self.get_user_output_path())
            
            if result.get('error'):
                self.logger.error(f"âŒ ê·¸ë£¹ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error')}")
                return {'data': {}, 'error': result.get('error')}
            
            # 4. prev_stage_resultì—ì„œ ì§ì ‘ ì¥ë³„ íŒŒì¼ëª… ê·¸ë£¹í™”
            chapter_files = self._extract_chapter_files_from_prev_result(prev_stage_result)
            
            # 5. book_title ì •ë³´ ì¶”ê°€
            book_information = prev_stage_result.get('data', {}).get('book_information', {})
            book_title = book_information.get('title', 'Unknown Book')
            
            self.logger.info(f"ğŸ‰ ContentProcessingStage ì™„ë£Œ")
            
            return {
                'data': {
                    'book_title': book_title,
                    'chapter_info_docs': chapter_files
                }, 
                'error': None
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ContentProcessingStage ì‹¤íŒ¨: {e}")
            return {'data': {}, 'error': str(e)}

    async def generate_enhanced_toc_file(self, book_folder_path: str) -> bool:
        """ğŸ“– ê°œì„ ëœ ëª©ì°¨ MD íŒŒì¼ ìƒì„±"""
        try:
            # 1. TOC êµ¬ì¡° íŒŒì¼ ë¡œë“œ
            chapter_name = os.path.basename(book_folder_path)
            toc_file_path = os.path.join(book_folder_path, f"{chapter_name}_toc.json")
            
            if not os.path.exists(toc_file_path):
                self.logger.warning(f"âš ï¸ TOC íŒŒì¼ ì—†ìŒ: {toc_file_path}")
                return False
            
            with open(toc_file_path, 'r', encoding='utf-8') as f:
                toc_structure = json.load(f)
            
            self.logger.info(f"ğŸ“‹ TOC êµ¬ì¡° ë¡œë“œ: {len(toc_structure)}ê°œ í•­ëª©")
            
            # 2. ëª¨ë“  í†µí•© ë¬¸ì„œ ë¡œë“œ ë° ë§¤ì¹­
            unified_docs_dir = os.path.join(book_folder_path, "unified_info_docs")
            all_docs = {}
            
            for file_path in glob.glob(f"{unified_docs_dir}/*_info.md"):
                doc_data = await self.parse_unified_document(file_path)
                if doc_data and doc_data.get('title'):
                    # ì œëª©ì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ë§¤ì¹­
                    all_docs[doc_data['title']] = doc_data
            
            self.logger.info(f"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ: {len(all_docs)}ê°œ")
            
            # 3. TOC êµ¬ì¡°ì— ë”°ë¼ MD íŒŒì¼ ìƒì„±
            enhanced_lines = []
            matched_count = 0
            
            for toc_item in toc_structure:
                title = toc_item.get('title', '')
                level = toc_item.get('level', 1)
                
                # í—¤ë” ìƒì„±
                header_prefix = "#" * level
                header = f"{header_prefix} {title}"
                
                # ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œ ì°¾ê¸°
                matched_doc = self.find_matching_document(all_docs, toc_item)
                
                if matched_doc:
                    extraction_content = self.get_extracted_information(matched_doc)
                    if extraction_content.strip():
                        enhanced_lines.append(f"{header}\n{extraction_content}")
                        matched_count += 1
                    else:
                        enhanced_lines.append(f"{header}\n[ì¶”ì¶œ ë‚´ìš© ì—†ìŒ]")
                else:
                    enhanced_lines.append(f"{header}\n[ë§¤ì¹­ ë¬¸ì„œ ì—†ìŒ]")
                
                # ì„¹ì…˜ ê°„ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ ì¤„ ì¶”ê°€
                enhanced_lines.append("")
                enhanced_lines.append("")
            
            # 4. íŒŒì¼ ì €ì¥
            output_file = os.path.join(book_folder_path, f"{chapter_name}_enhanced_ToC.md")
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(enhanced_lines))
            
            self.logger.info(f"âœ… ê°œì„ ëœ TOC íŒŒì¼ ìƒì„±: {output_file}")
            self.logger.info(f"ğŸ“Š ë§¤ì¹­ëœ ë¬¸ì„œ: {matched_count}/{len(toc_structure)}ê°œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ TOC íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def find_matching_document(self, all_docs: Dict[str, Dict], toc_item: Dict) -> Optional[Dict]:
        """ğŸ“ TOC í•­ëª©ê³¼ ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œ ì°¾ê¸° - text_utils.normalize_title í™œìš©"""
        toc_title = toc_item.get('title', '').strip()
        
        # 1. ì •í™•í•œ ì œëª© ë§¤ì¹­
        if toc_title in all_docs:
            return all_docs[toc_title]
        
        # 2. normalize_title í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì •ê·œí™” ë§¤ì¹­
        normalized_toc_title = normalize_title(toc_title)
        
        for doc_title, doc_data in all_docs.items():
            normalized_doc_title = normalize_title(doc_title)
            
            # ì •ê·œí™”ëœ ì œëª©ì— TOC ì •ê·œí™” ì œëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if normalized_toc_title and normalized_toc_title in normalized_doc_title:
                self.logger.info(f"ğŸ¯ ì •ê·œí™” ë§¤ì¹­ ì„±ê³µ: '{toc_title}' â†’ '{doc_title}'")
                self.logger.info(f"    ì •ê·œí™”ëœ TOC: '{normalized_toc_title}'")
                self.logger.info(f"    ì •ê·œí™”ëœ ë¬¸ì„œ: '{normalized_doc_title}'")
                return doc_data
            
            # ì—­ë°©í–¥ ë§¤ì¹­ë„ ì‹œë„
            elif normalized_doc_title and normalized_doc_title in normalized_toc_title:
                self.logger.info(f"ğŸ¯ ì—­ì •ê·œí™” ë§¤ì¹­ ì„±ê³µ: '{toc_title}' â†’ '{doc_title}'")
                self.logger.info(f"    ì •ê·œí™”ëœ TOC: '{normalized_toc_title}'")
                self.logger.info(f"    ì •ê·œí™”ëœ ë¬¸ì„œ: '{normalized_doc_title}'")
                return doc_data
        
        # 3. ë¶€ë¶„ í‚¤ì›Œë“œ ë§¤ì¹­ (ì •ê·œí™”ëœ ì œëª©ì„ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë§¤ì¹­)
        if normalized_toc_title:
            toc_words = set(normalized_toc_title.split('_'))  # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë¶„ë¦¬
            toc_words.discard('')  # ë¹ˆ ë¬¸ìì—´ ì œê±°
            
            best_match = None
            best_score = 0
            
            for doc_title, doc_data in all_docs.items():
                normalized_doc_title = normalize_title(doc_title)
                if normalized_doc_title:
                    doc_words = set(normalized_doc_title.split('_'))
                    doc_words.discard('')
                    
                    if len(toc_words) > 0 and len(doc_words) > 0:
                        common_words = toc_words.intersection(doc_words)
                        score = len(common_words) / len(toc_words)
                        
                        if score > best_score and score >= 0.5:  # 50% ì´ìƒ ì¼ì¹˜
                            best_score = score
                            best_match = doc_data
            
            if best_match:
                self.logger.info(f"ğŸ¯ í‚¤ì›Œë“œ ë§¤ì¹­ ì„±ê³µ: '{toc_title}' â†’ '{best_match.get('title')}' (ì¼ì¹˜ìœ¨: {best_score:.1%})")
                return best_match
        
        # 4. ë§¤ì¹­ ì‹¤íŒ¨
        self.logger.warning(f"âŒ ë§¤ì¹­ ì‹¤íŒ¨: '{toc_title}' (ì •ê·œí™”: '{normalized_toc_title}')")
        return None

    def get_extracted_information(self, doc_data: Dict) -> str:
        """ğŸ“ ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ"""
        extraction_section = doc_data.get('extraction_section', '').strip()
        
        if not extraction_section or extraction_section == '---':
            return "[ì¶”ì¶œ ë‚´ìš© ì—†ìŒ]"
        
        # ëª¨ë“  ì¶”ì¶œ ì„¹ì…˜ í¬í•¨ (í•µì‹¬ ë‚´ìš©, ìƒì„¸ í•µì‹¬ ë‚´ìš©, ìƒì„¸ ì •ë³´, ì£¼ìš” í™”ì œ, ë¶€ì°¨ í™”ì œ)
        sections = self.parse_extraction_sections(extraction_section)
        
        selected_information_type = []
        for section_name in ['core_content', 'detailed_core_content', 'detailed_content', 'main_topics', 'sub_topics']:
            if section_name in sections and sections[section_name].strip():
                content = sections[section_name].strip()
                # í—¤ë” ì¶”ê°€í•˜ì—¬ ì„¹ì…˜ êµ¬ë¶„
                if section_name == 'core_content':
                    selected_information_type.append(f"## í•µì‹¬ ë‚´ìš©\n{content}")
                elif section_name == 'detailed_core_content':
                    selected_information_type.append(f"## ìƒì„¸ í•µì‹¬ ë‚´ìš©\n{content}")
                elif section_name == 'detailed_content':
                    selected_information_type.append(f"## ìƒì„¸ ì •ë³´\n{content}")
                elif section_name == 'main_topics':
                    selected_information_type.append(f"## ì£¼ìš” í™”ì œ\n{content}")
                elif section_name == 'sub_topics':
                    selected_information_type.append(f"## ë¶€ì°¨ í™”ì œ\n{content}")
        
        if selected_information_type:
            return '\n\n'.join(selected_information_type)
        else:
            return extraction_section[:500] + "..." if len(extraction_section) > 500 else extraction_section

    def parse_extraction_sections(self, extraction_content: str) -> Dict[str, str]:
        """ğŸ“‹ ì¶”ì¶œ ì„¹ì…˜ì„ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        sections = {}
        current_section = None
        current_content = []
        
        for line in extraction_content.split('\n'):
            line = line.strip()
            
            # ì„¹ì…˜ í—¤ë” ê°ì§€ (## ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸)
            if line.startswith('## '):
                # ì´ì „ ì„¹ì…˜ ì €ì¥ (í—¤ë” ì œì™¸í•˜ê³  ë‚´ìš©ë§Œ)
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                section_title = line[3:].strip()  # ## ì œê±°
                # ë” êµ¬ì²´ì ì¸ íŒ¨í„´ì„ ë¨¼ì € í™•ì¸
                if 'ìƒì„¸ í•µì‹¬' in section_title:
                    current_section = 'detailed_core_content'
                elif 'í•µì‹¬ ë‚´ìš©' in section_title:
                    current_section = 'core_content'
                elif 'ìƒì„¸ ì •ë³´' in section_title:
                    current_section = 'detailed_content'
                elif 'ì£¼ìš” í™”ì œ' in section_title:
                    current_section = 'main_topics'
                elif 'ë¶€ì°¨ í™”ì œ' in section_title:
                    current_section = 'sub_topics'
                else:
                    current_section = None
                
                current_content = []  # í—¤ë”ëŠ” ì €ì¥í•˜ì§€ ì•Šê³  ë‚´ìš©ë§Œ ì €ì¥
            elif current_section:
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections

    async def save_extraction_result(self, doc: Dict, extraction_result: Dict[str, str], user_output_path: str):
        """ğŸ“ ëª¨ë“  ë…¸ë“œì˜ ì¶”ì¶œ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì§€ì • ê²½ë¡œì— ì €ì¥ (ê³µí†µ ë¡œì§)"""
        if not extraction_result:
            self.logger.warning(f"âš ï¸ ì €ì¥í•  ì¶”ì¶œ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ: {doc.get('title', 'Unknown')}")
            return
        
        try:
            # 1. íŒŒì¼ ê²½ë¡œ êµ¬ì„± ë° ë””ë ‰í„°ë¦¬ ìƒì„±
            doc_title = doc.get('title', 'Unknown')
            original_file_name = doc.get('file_name', f"{doc_title.replace(' ', '_')}_info.md")
            
            # file_name êµ¬ì¡°: {ì±…ì´ë¦„}/{ì¥}/{í†µí•©ë¬¸ì„œíŒŒì¼ëª…}
            # ì €ì¥ êµ¬ì¡°: user_output_path/{ì±…ì´ë¦„}/{ì¥}/{í†µí•©ë¬¸ì„œíŒŒì¼ëª…}
            if '/' in original_file_name:
                # ì „ì²´ ê²½ë¡œ êµ¬ì¡° ìœ ì§€ (ì±…ì´ë¦„/ì¥/íŒŒì¼ëª…)
                relative_path = Path(original_file_name)
                output_dir = Path(user_output_path) / relative_path.parent
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file_path = output_dir / relative_path.name
            else:
                # ê²½ë¡œê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ì§€ì • ê²½ë¡œì— ì§ì ‘ ì €ì¥
                output_dir = Path(user_output_path)
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file_path = output_dir / original_file_name
            
            self.logger.info(f"ğŸ“ ì¶”ì¶œ ê²°ê³¼ ì €ì¥ ì‹œì‘: {output_file_path}")
            
            # 2. ì¶”ì¶œ ì„¹ì…˜ í¬ë§·íŒ…
            formatted_content = combine_extraction_sections(extraction_result)
            
            # 3. ê¸°ì¡´ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (docì˜ full_content ì‚¬ìš©)
            original_content = doc.get('full_content', '')
            
            if original_content:
                # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ì´ ìˆìœ¼ë©´ êµì²´, ì—†ìœ¼ë©´ ì¶”ê°€
                extraction_pattern = r'(# ì¶”ì¶œ\n---\n)(.*?)(?=\n# |$)'
                
                if re.search(extraction_pattern, original_content, re.DOTALL):
                    # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ êµì²´
                    new_content = re.sub(
                        extraction_pattern,
                        f'\\1{formatted_content}\n',
                        original_content,
                        flags=re.DOTALL
                    )
                else:
                    # ì¶”ì¶œ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ # ë‚´ìš© ì•ì— ì¶”ê°€
                    content_pattern = r'(\n# ë‚´ìš©)'
                    if re.search(content_pattern, original_content):
                        new_content = re.sub(
                            content_pattern,
                            f'\n# ì¶”ì¶œ\n---\n{formatted_content}\n\\1',
                            original_content
                        )
                    else:
                        # # ë‚´ìš© ì„¹ì…˜ë„ ì—†ìœ¼ë©´ íŒŒì¼ ëì— ì¶”ê°€
                        new_content = original_content + f'\n\n# ì¶”ì¶œ\n---\n{formatted_content}\n'
            else:
                # ì›ë³¸ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì¶”ì¶œ ê²°ê³¼ë§Œ ì €ì¥
                new_content = f'# ì¶”ì¶œ\n---\n{formatted_content}\n'
            
            # 4. íŒŒì¼ ì €ì¥
            with open(output_file_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            self.logger.info(f"âœ… ì¶”ì¶œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file_path.name}")
            return str(output_file_path)
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶”ì¶œ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {doc.get('title', 'Unknown')} - {e}")
            raise

    async def update_current_extraction_section(self, doc: Dict, user_output_path: str) -> tuple:
        """
        í˜„ì¬ ë¹„ë¦¬í”„ ë…¸ë“œì˜ ì¶”ì¶œ ì„¹ì…˜ì„ êµ¬ì„± íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        
        Args:
            doc: ë¹„ë¦¬í”„ ë…¸ë“œ ë¬¸ì„œ ì •ë³´
            user_output_path: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ê²½ë¡œ
            
        Returns:
            tuple: (updated_current_extraction: Dict, used_composition_extractions: str)
            
        ì²˜ë¦¬ ê³¼ì •:
        1. ì‚¬ìš©ì ì§€ì • ê²½ë¡œì—ì„œ í˜„ì¬ ë…¸ë“œ íŒŒì¼ ì½ê¸°
        2. êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ ìˆ˜ì§‘ (íŒŒì¼ì—ì„œ ì½ê¸°)
        3. AI ì„œë¹„ìŠ¤ë¡œ ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        4. ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì„ íŒŒì¼ì— ì €ì¥
        5. ëª…ì‹œì ìœ¼ë¡œ ë‘ ê°’ ë°˜í™˜
        """
        try:
            self.logger.info(f"ğŸ”„ í˜„ì¬ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹œì‘: {doc.get('title', 'Unknown')}")
            
            # 1. í˜„ì¬ ë…¸ë“œ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
            file_name = doc.get('file_name', '')
            # {user_output_path}/{ì „ì²´ê²½ë¡œ}ë¡œ êµ¬ì„±
            current_file_path = Path(user_output_path) / file_name
            
            if not current_file_path.exists():
                raise FileNotFoundError(f"í˜„ì¬ ë…¸ë“œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {current_file_path}")
            
            # 2. í˜„ì¬ ë…¸ë“œì˜ ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ì½ê¸°
            with open(current_file_path, 'r', encoding='utf-8') as f:
                current_content = f.read()
            
            current_extraction = self._parse_extraction_section_from_content(current_content)
            self.logger.info(f"ğŸ“„ í˜„ì¬ ì¶”ì¶œ ì„¹ì…˜ ë¡œë“œ: {len(current_extraction)} ì„¹ì…˜")
            
            # 3. êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ ìˆ˜ì§‘
            composition_extractions = []
            composition_files = doc.get('composition_files', [])
            
            # file_nameì—ì„œ ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì¶”ì¶œ ({ì±…í´ë”}/{ì¥í´ë”})
            file_name = doc.get('file_name', '')
            file_dir = Path(file_name).parent  # Data_Oriented_Programming/1_Complexity_of_object_oriented_programming/unified_info_docs
            
            for comp_file in composition_files:
                # {user_output_path}/{ì±…í´ë”}/{ì¥í´ë”}/{êµ¬ì„±íŒŒì¼ëª…}
                comp_file_path = Path(user_output_path) / file_dir / comp_file
                if comp_file_path.exists():
                    with open(comp_file_path, 'r', encoding='utf-8') as f:
                        comp_content = f.read()
                    
                    comp_extraction = self._parse_extraction_section_from_content(comp_content)
                    if comp_extraction:
                        # ì»´í¬ì§€ì…˜ ì •ë³´ êµ¬ì„±
                        comp_title = self._extract_title_from_filename(comp_file)
                        composition_extractions.append({
                            'title': comp_title,
                            'extraction': comp_extraction
                        })
                        self.logger.info(f"âœ… êµ¬ì„± íŒŒì¼ ì¶”ì¶œ ì„¹ì…˜ ë¡œë“œ: {comp_file}")
                    else:
                        self.logger.warning(f"âš ï¸ êµ¬ì„± íŒŒì¼ ì¶”ì¶œ ì„¹ì…˜ ì—†ìŒ: {comp_file}")
                else:
                    self.logger.warning(f"âš ï¸ êµ¬ì„± íŒŒì¼ ì—†ìŒ: {comp_file_path}")
            
            if not composition_extractions:
                raise ValueError("ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì„± ë…¸ë“œ ì¶”ì¶œ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # 4. AI ì„œë¹„ìŠ¤ë¡œ ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸ ìˆ˜í–‰ (engines_v5.py ë¡œì§ í™œìš©)
            updated_current_extraction = await self._update_parent_with_composition_logic(
                current_doc=doc,
                current_extraction=current_extraction,
                composition_extractions=composition_extractions
            )
            
            # 5. êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ì„ ë¬¸ìì—´ë¡œ ê²°í•©
            used_composition_extractions = self._combine_composition_extractions(composition_extractions)
            
            # 6. ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì„ íŒŒì¼ì— ì €ì¥ (ìƒíƒœ ë§ˆí‚¹ í¬í•¨)
            await self._save_updated_extraction_to_file(current_file_path, updated_current_extraction, "<êµ¬ì„± ë…¸ë“œ ë°˜ì˜ ì™„ë£Œ>")
            
            self.logger.info(f"âœ… í˜„ì¬ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {doc.get('title', 'Unknown')}")
            
            return updated_current_extraction, used_composition_extractions
            
        except Exception as e:
            self.logger.error(f"âŒ í˜„ì¬ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {doc.get('title', 'Unknown')} - {e}")
            raise

    def _parse_extraction_section_from_content(self, content: str) -> Dict:
        """íŒŒì¼ ë‚´ìš©ì—ì„œ ì¶”ì¶œ ì„¹ì…˜ì„ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
        try:
            # # ì¶”ì¶œ ì„¹ì…˜ ì°¾ê¸°
            extraction_match = re.search(r'# ì¶”ì¶œ\n---\n(.*?)(?=\n# |$)', content, re.DOTALL)
            if not extraction_match:
                return {}
            
            extraction_content = extraction_match.group(1).strip()
            if not extraction_content:
                return {}
            
            # engines_v5.pyì˜ parse_extraction_response ë¡œì§ í™œìš©
            return parse_extraction_response(extraction_content)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì¶”ì¶œ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {}

    def _extract_title_from_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ"""
        try:
            # ì˜ˆ: "17_lev3_1.1.1_The_design_phase_info.md" -> "17 lev3 1.1.1 The design phase"
            title_match = re.search(r'(\d+_lev\d+_.*?)_info\.md', filename)
            if title_match:
                return title_match.group(1).replace('_', ' ')
            return filename.replace('_info.md', '').replace('_', ' ')
        except Exception:
            return filename

    async def _update_parent_with_composition_logic(self, current_doc: Dict, current_extraction: Dict, composition_extractions: List[Dict]) -> Dict:
        """engines_v5.pyì˜ update_parent_extraction_with_composition ë¡œì§ í™œìš©í•˜ì—¬ ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸"""
        try:
            # í˜„ì¬ ì¶”ì¶œ ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ
            parent_core = current_extraction.get('core_content', '').replace('## í•µì‹¬ ë‚´ìš©', '').strip()
            parent_detailed_core = current_extraction.get('detailed_core_content', '').replace('## ìƒì„¸ í•µì‹¬ ë‚´ìš©', '').strip()
            parent_detailed_info = current_extraction.get('detailed_content', '').replace('## ìƒì„¸ ì •ë³´', '').strip()
            parent_main_topics = current_extraction.get('main_topics', '').replace('## ì£¼ìš” í™”ì œ', '').strip()
            parent_sub_topics = current_extraction.get('sub_topics', '').replace('## ë¶€ì°¨ í™”ì œ', '').strip()
            
            # êµ¬ì„± ì •ë³´ í¬ë§·íŒ… (engines_v5.pyì™€ ë™ì¼)
            composition_info = []
            for comp in composition_extractions:
                comp_sections = comp['extraction']
                comp_core = comp_sections.get('core_content', '').replace('## í•µì‹¬ ë‚´ìš©', '').strip()
                comp_detailed_core = comp_sections.get('detailed_core_content', '').replace('## ìƒì„¸ í•µì‹¬ ë‚´ìš©', '').strip()
                comp_detailed_info = comp_sections.get('detailed_content', '').replace('## ìƒì„¸ ì •ë³´', '').strip()
                comp_main_topics = comp_sections.get('main_topics', '').replace('## ì£¼ìš” í™”ì œ', '').strip()
                comp_sub_topics = comp_sections.get('sub_topics', '').replace('## ë¶€ì°¨ í™”ì œ', '').strip()
                
                child_info = f"""
êµ¬ì„±ë…¸ë“œ ({comp['title']}):
- í•µì‹¬ ë‚´ìš©: {comp_core}
- ìƒì„¸ í•µì‹¬ ë‚´ìš©: {comp_detailed_core}
- ìƒì„¸ ì •ë³´: {comp_detailed_info}
- ì£¼ìš” í™”ì œ: {comp_main_topics}
- ë¶€ì°¨ í™”ì œ: {comp_sub_topics}"""
                
                composition_info.append(child_info)
            
            # engines_v5.py í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì‚¬ìš©
            prompt = f"""ë‹¤ìŒì€ ë¶€ëª¨ ë…¸ë“œì˜ ì¶”ì¶œ ì„¹ì…˜ì„ êµ¬ì„± ë…¸ë“œë“¤ì˜ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ì—…ë°ì´íŠ¸í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.

**ë¶€ëª¨ ë…¸ë“œ ({current_doc.get('title', 'Unknown')})ì˜ í˜„ì¬ ë‚´ìš©:**
í•µì‹¬ ë‚´ìš©: {parent_core}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {parent_detailed_core}
ìƒì„¸ ì •ë³´: {parent_detailed_info}
ì£¼ìš” í™”ì œ: {parent_main_topics}
ë¶€ì°¨ í™”ì œ: {parent_sub_topics}

**êµ¬ì„± ë…¸ë“œë“¤ì˜ ë‚´ìš©:**
{chr(10).join(composition_info)}

ë¶€ëª¨ ë…¸ë“œì˜ ê° ì„¹ì…˜ì„ êµ¬ì„± ë…¸ë“œë“¤ì˜ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ê°œì„ í•´ì£¼ì„¸ìš”. 
ë¶€ëª¨ ë…¸ë“œëŠ” ì „ì²´ì ì¸ ê°œìš”ì™€ í†µí•©ì ì¸ ê´€ì ì„ ì œê³µí•˜ë˜, êµ¬ì„± ë…¸ë“œë“¤ì˜ ì„¸ë¶€ ë‚´ìš©ì´ ì˜ ë°˜ì˜ë˜ë„ë¡ í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ 5ê°œ ì„¹ì…˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

## í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ í•µì‹¬ ë‚´ìš©  
[ê°œì„ ëœ ìƒì„¸ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ ì •ë³´
[ê°œì„ ëœ ìƒì„¸ ì •ë³´]

## ì£¼ìš” í™”ì œ
[ê°œì„ ëœ ì£¼ìš” í™”ì œ]

## ë¶€ì°¨ í™”ì œ
[ê°œì„ ëœ ë¶€ì°¨ í™”ì œ]"""
            
            # AI ì„œë¹„ìŠ¤ í˜¸ì¶œ
            response = await self.ai_service.query_single_request(prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            parsed_response = parse_extraction_response(response)
            
            if len(parsed_response) >= 3:  # ìµœì†Œí•œ í•µì‹¬ 3ê°œ ì„¹ì…˜ì€ ìˆì–´ì•¼ í•¨
                self.logger.info(f"âœ… ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸ ì„±ê³µ: {len(parsed_response)} ì„¹ì…˜")
                return parsed_response
            else:
                self.logger.warning("âš ï¸ AI ì‘ë‹µ í’ˆì§ˆ ë¶€ì¡±, ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ìœ ì§€")
                return current_extraction
                
        except Exception as e:
            self.logger.error(f"âŒ ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return current_extraction

    def _combine_composition_extractions(self, composition_extractions: List[Dict]) -> str:
        """êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©"""
        try:
            combined_parts = []
            for comp in composition_extractions:
                comp_title = comp['title']
                comp_sections = comp['extraction']
                
                # ê° êµ¬ì„± ë…¸ë“œì˜ ì¶”ì¶œ ì„¹ì…˜ì„ í¬ë§·íŒ…
                section_text = f"=== {comp_title} ===\n"
                for section_content in comp_sections.values():
                    section_text += f"{section_content}\n"
                
                combined_parts.append(section_text)
            
            return "\n".join(combined_parts)
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ êµ¬ì„± ì¶”ì¶œ ì„¹ì…˜ ê²°í•© ì‹¤íŒ¨: {e}")
            return ""

    async def _save_updated_extraction_to_file(self, file_path: Path, updated_extraction: Dict, status_marker: str):
        """ì—…ë°ì´íŠ¸ëœ ì¶”ì¶œ ì„¹ì…˜ì„ íŒŒì¼ì— ì €ì¥ (ìƒíƒœ ë§ˆí‚¹ í¬í•¨)"""
        try:
            # ìƒˆë¡œìš´ ì¶”ì¶œ ì„¹ì…˜ ë‚´ìš© í¬ë§·íŒ… - ìƒíƒœ ë§ˆí‚¹ í¬í•¨
            formatted_extraction = combine_extraction_sections(updated_extraction)
            # ìƒíƒœ ë§ˆí‚¹ì„ ì¶”ì¶œ ì„¹ì…˜ ë§¨ ì•ì— ì¶”ê°€
            formatted_extraction = f"{status_marker}\n\n{formatted_extraction}"
            
            # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ êµì²´ (update_extraction_sectionì€ boolean ë°˜í™˜)
            success = update_extraction_section(str(file_path), formatted_extraction)
            
            if success:
                self.logger.info(f"ğŸ’¾ ì—…ë°ì´íŠ¸ëœ ì¶”ì¶œ ì„¹ì…˜ ì €ì¥ ì™„ë£Œ: {file_path.name}")
            else:
                raise Exception("ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ê°€ ì‹¤íŒ¨ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
        except Exception as e:
            self.logger.error(f"âŒ ì—…ë°ì´íŠ¸ëœ ì¶”ì¶œ ì„¹ì…˜ ì €ì¥ ì‹¤íŒ¨: {file_path} - {e}")
            raise

    async def update_composition_extraction_sections(self, 
                                                   parent_doc: Dict,
                                                   parent_extraction: Dict,
                                                   used_composition_extractions: str,
                                                   composition_files: List[str],
                                                   user_output_path: str) -> None:
        """
        êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ì„ ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°˜ì˜í•˜ì—¬ ì¼ê´„ ì—…ë°ì´íŠ¸
        engines_v5.py íŒ¨í„´: í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ëª¨ë“  êµ¬ì„± ë…¸ë“œ ì—…ë°ì´íŠ¸
        
        Args:
            parent_doc: ë¶€ëª¨ ë…¸ë“œ ë¬¸ì„œ (filename ì •ë³´ í¬í•¨)
            parent_extraction: ì—…ë°ì´íŠ¸ëœ ë¶€ëª¨ ë…¸ë“œ ì¶”ì¶œ ì„¹ì…˜ 
            used_composition_extractions: ì‚¬ìš©ëœ êµ¬ì„± ë…¸ë“œë“¤ì˜ ê²°í•©ëœ ì¶”ì¶œ ì„¹ì…˜
            composition_files: êµ¬ì„± íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
            user_output_path: ì‚¬ìš©ì ì§€ì • ì €ì¥ ê²½ë¡œ
        """
        self.logger.info(f"ğŸ”„ êµ¬ì„± ë…¸ë“œ ì¼ê´„ ì—…ë°ì´íŠ¸ ì‹œì‘")
        
        if not composition_files:
            self.logger.info("êµ¬ì„± íŒŒì¼ì´ ì—†ì–´ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            return
            
        self.logger.info(f"ğŸ“ ì²˜ë¦¬í•  êµ¬ì„± íŒŒì¼ ìˆ˜: {len(composition_files)}ê°œ")
        
        try:
            # 1ë‹¨ê³„: í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ëª¨ë“  êµ¬ì„± ë…¸ë“œ ì—…ë°ì´íŠ¸
            response = await self._update_all_composition_sections(
                parent_extraction=parent_extraction,
                used_composition_extractions=used_composition_extractions,
                composition_files=composition_files
            )
            
            # 2ë‹¨ê³„: AI ì‘ë‹µ íŒŒì‹±
            node_sections = await self._parse_ai_response_to_node_sections(response)
            
            # 3ë‹¨ê³„: ê° êµ¬ì„± ë…¸ë“œ ê°œë³„ ì €ì¥
            await self._save_each_composition_node(
                node_sections=node_sections,
                parent_doc=parent_doc,
                composition_files=composition_files,
                user_output_path=user_output_path
            )
            
            self.logger.info(f"ğŸ‰ êµ¬ì„± ë…¸ë“œ ì¼ê´„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(composition_files)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ë…¸ë“œ ì¼ê´„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _update_all_composition_sections(self, 
                                             parent_extraction: Dict,
                                             used_composition_extractions: str,
                                             composition_files: List[str]) -> str:
        """
        engines_v5.py íŒ¨í„´: í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ëª¨ë“  êµ¬ì„± ë…¸ë“œì˜ í•µì‹¬ 3ê°œ ì„¹ì…˜ ì—…ë°ì´íŠ¸
        """
        # ë¶€ëª¨ ë…¸ë“œì˜ í•µì‹¬ 3ê°œ ì„¹ì…˜ë§Œ ì¶”ì¶œ (engines_v5.py ë™ì¼)
        parent_core = parent_extraction.get('core_content', '').replace('## í•µì‹¬ ë‚´ìš©', '').strip()
        parent_detailed_core = parent_extraction.get('detailed_core_content', '').replace('## ìƒì„¸ í•µì‹¬ ë‚´ìš©', '').strip()
        parent_detailed_info = parent_extraction.get('detailed_content', '').replace('## ìƒì„¸ ì •ë³´', '').strip()
        
        # êµ¬ì„± íŒŒì¼ ìˆ˜ ì •ë³´ ì¶”ê°€ë¡œ AI ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
        composition_count = len(composition_files)
        
        # engines_v5.pyì—ì„œ ìˆ˜ì • ìš”ì²­ëœ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ - êµ¬ì„± íŒŒì¼ ìˆ˜ ëª…ì‹œ
        prompt = f"""ë‹¤ìŒì€ ë¶€ëª¨ ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì´ {composition_count}ê°œ** êµ¬ì„± ë…¸ë“œë“¤ì˜ í•µì‹¬ 3ê°€ì§€ ì •ë³´ ì„¹ì…˜ë§Œ ê°œì„ í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.

**ë¶€ëª¨ ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©:**
í•µì‹¬ ë‚´ìš©: {parent_core}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {parent_detailed_core}
ìƒì„¸ ì •ë³´: {parent_detailed_info}

**êµ¬ì„± ë…¸ë“œë“¤ì˜ í˜„ì¬ ë‚´ìš©:**
{used_composition_extractions}

ë¶€ëª¨ ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ê° êµ¬ì„± ë…¸ë“œì˜ **3ê°€ì§€ ì •ë³´ ì„¹ì…˜(í•µì‹¬ ë‚´ìš©, ìƒì„¸ í•µì‹¬ ë‚´ìš©, ìƒì„¸ ì •ë³´)ë§Œ** ê°œì„ í•´ì£¼ì„¸ìš”.
ê° êµ¬ì„± ë…¸ë“œì˜ ê³ ìœ í•œ íŠ¹ì„±ì€ ìœ ì§€í•˜ë˜, ë¶€ëª¨ì™€ì˜ ì¼ê´€ì„±ê³¼ ì—°ê²°ì„±ì„ ë°˜ì˜í•´ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ {composition_count}ê°œ ëª¨ë“  êµ¬ì„±ë…¸ë“œì— ëŒ€í•´ ì‘ë‹µí•´ì£¼ì„¸ìš”.**

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

êµ¬ì„±ë…¸ë“œ1:
## í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ ìƒì„¸ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ ì •ë³´
[ê°œì„ ëœ ìƒì„¸ ì •ë³´]

êµ¬ì„±ë…¸ë“œ2:
## í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ ìƒì„¸ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ ì •ë³´
[ê°œì„ ëœ ìƒì„¸ ì •ë³´]

**ì¤‘ìš”**: ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ "## " (í•´ì‹œ 2ê°œ + ê³µë°±)ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì œëª©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."""

        # ë‹¨ì¼ AI í˜¸ì¶œ
        response = await self.ai_service.query_single_request(prompt)
        self.logger.info(f"âœ… AI ì¼ê´„ í˜¸ì¶œ ì™„ë£Œ")
        
        return response

    async def _parse_ai_response_to_node_sections(self, response: str) -> List[Dict[str, str]]:
        """
        AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹± (SRP: íŒŒì‹±ë§Œ ë‹´ë‹¹)
        """
        try:
            # AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹±
            node_sections = self._parse_ai_response_for_composition(response)
            
            self.logger.info(f"ğŸ” ìµœì¢… íŒŒì‹± ê²°ê³¼: {len(node_sections)}ê°œ ë…¸ë“œ")
            return node_sections
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise

    async def _save_each_composition_node(self, 
                                       node_sections: List[Dict[str, str]],
                                       parent_doc: Dict,
                                       composition_files: List[str],
                                       user_output_path: str) -> None:
        """
        íŒŒì‹±ëœ ë…¸ë“œ ì„¹ì…˜ë“¤ì„ ê°ê° ê°œë³„ ì €ì¥ (SRP: ì €ì¥ë§Œ ë‹´ë‹¹)
        """
        try:
            # ê²€ì¦: íŒŒì‹±ëœ ì„¹ì…˜ ìˆ˜ì™€ êµ¬ì„± íŒŒì¼ ìˆ˜ ë¹„êµ
            expected_count = len(composition_files)
            parsed_count = len(node_sections)
            
            self.logger.info(f"ğŸ“Š AI ì‘ë‹µ íŒŒì‹± ê²°ê³¼: ì˜ˆìƒ {expected_count}ê°œ, íŒŒì‹± {parsed_count}ê°œ")
            
            if parsed_count < expected_count:
                self.logger.warning(f"âš ï¸ AI ì‘ë‹µì—ì„œ {expected_count - parsed_count}ê°œ êµ¬ì„±ë…¸ë“œ ì„¹ì…˜ ëˆ„ë½")
            elif parsed_count == expected_count:
                self.logger.info("âœ… AI ì‘ë‹µ íŒŒì‹± ì™„ë£Œ: ëª¨ë“  ì„¹ì…˜ ì •ìƒ")
            
            successful_updates = 0
            
            # ê° êµ¬ì„± íŒŒì¼ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
            for i, comp_file in enumerate(composition_files):
                try:
                    if i >= len(node_sections):
                        self.logger.warning(f"âš ï¸ AI ì‘ë‹µì—ì„œ {comp_file}ì— í•´ë‹¹í•˜ëŠ” ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        continue
                    
                    # íŒŒì¼ ê²½ë¡œ êµ¬ì„±
                    comp_file_path = self._get_composition_file_path(comp_file, user_output_path, parent_doc)
                    
                    # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ë¡œë“œ (ì£¼ìš”/ë¶€ì°¨ í™”ì œ ë³´ì¡´ìš©)
                    existing_extraction = await self._load_parent_topic_extractions(comp_file_path)
                    
                    # ì—…ë°ì´íŠ¸ëœ ì„¹ì…˜ê³¼ ê¸°ì¡´ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ê²°í•©
                    final_sections = self._merge_with_preserved_topics(node_sections[i], existing_extraction)
                    
                    # ê°œë³„ ì €ì¥ (ê¸°ì¡´ ë¡œì§ ì¬í™œìš©)
                    await self._save_updated_extraction_to_file(
                        file_path=comp_file_path,
                        updated_extraction=final_sections,
                        status_marker="<ë¶€ëª¨ ë…¸ë“œ ë°˜ì˜ ì™„ë£Œ>"
                    )
                    
                    successful_updates += 1
                    self.logger.info(f"âœ… êµ¬ì„± ë…¸ë“œ ì €ì¥ ì™„ë£Œ: {comp_file}")
                    
                except Exception as e:
                    self.logger.error(f"âŒ êµ¬ì„± ë…¸ë“œ {comp_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“Š êµ¬ì„± ë…¸ë“œ ê°œë³„ ì €ì¥ ì™„ë£Œ: {successful_updates}/{len(composition_files)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ë° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def _get_composition_file_path(self, comp_file: str, user_output_path: str, parent_doc: Dict) -> Path:
        """
        êµ¬ì„± íŒŒì¼ ê²½ë¡œ êµ¬ì„± (parent_doc file_nameì—ì„œ ê²½ë¡œ ì¶”ì¶œ)
        parent_docì˜ file_nameì—ì„œ {ì±…í´ë”}/{ì¥í´ë”}/unified_info_docs ê²½ë¡œë¥¼ ì¶”ì¶œí•˜ì—¬ í™œìš©
        """
        # parent_docì˜ file_nameì—ì„œ ê²½ë¡œ ì •ë³´ ì¶”ì¶œ
        parent_filename = parent_doc.get('file_name', '')
        if not parent_filename:
            raise ValueError("parent_docì— file_nameì´ ì—†ìŠµë‹ˆë‹¤")
        
        # file_nameì—ì„œ ë””ë ‰í† ë¦¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ (íŒŒì¼ëª… ì œì™¸)
        # ì˜ˆ: "Data_Oriented_Programming/1_Complexity_of_object_oriented_programming/unified_info_docs/16_lev2_1.1_OOP_design_Classic_or_classical_info.md"
        # -> "Data_Oriented_Programming/1_Complexity_of_object_oriented_programming/unified_info_docs"
        parent_dir = '/'.join(parent_filename.split('/')[:-1])
        
        comp_file_path = Path(user_output_path) / parent_dir / comp_file
        return comp_file_path

    async def _load_parent_topic_extractions(self, comp_file_path: Path) -> Dict[str, str]:
        """ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ì—ì„œ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ë¡œë“œ (ë³´ì¡´ìš©)"""
        try:
            if not comp_file_path.exists():
                raise FileNotFoundError(f"êµ¬ì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {comp_file_path}")
            
            with open(comp_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì¶”ì¶œ ì„¹ì…˜ ì¶”ì¶œ
            extraction_match = re.search(r'# ì¶”ì¶œ\n---\n(.*?)(?=\n# |$)', content, re.DOTALL)
            if not extraction_match:
                raise ValueError(f"ì¶”ì¶œ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {comp_file_path}")
                
            extraction_content = extraction_match.group(1).strip()
            if not extraction_content:
                raise ValueError(f"ì¶”ì¶œ ì„¹ì…˜ì´ ë¹„ì–´ìˆìŒ: {comp_file_path}")
                
            parsed_sections = parse_extraction_response(extraction_content)
            if not parsed_sections:
                raise ValueError(f"ì¶”ì¶œ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨: {comp_file_path}")
                
            return parsed_sections
            
        except Exception as e:
            self.logger.error(f"âŒ ë¶€ëª¨ í™”ì œ ì¶”ì¶œ ì‹¤íŒ¨: {comp_file_path} - {e}")
            raise RuntimeError(f"ë¶€ëª¨ í™”ì œ ì¶”ì¶œ ì‹¤íŒ¨: {comp_file_path} - {e}")

    def _merge_with_preserved_topics(self, updated_sections: Dict[str, str], existing_extraction: Dict[str, str]) -> Dict[str, str]:
        """
        ì—…ë°ì´íŠ¸ëœ í•µì‹¬ 3ê°œ ì„¹ì…˜ê³¼ ê¸°ì¡´ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ê²°í•©
        engines_v5.py ë³´ì¡´ ë¡œì§
        """
        return {
            'core_content': updated_sections.get('core_content', existing_extraction.get('core_content', '')),
            'detailed_core_content': updated_sections.get('detailed_core_content', existing_extraction.get('detailed_core_content', '')),
            'detailed_content': updated_sections.get('detailed_content', existing_extraction.get('detailed_content', '')),
            'main_topics': existing_extraction.get('main_topics', ''),      # ë³´ì¡´
            'sub_topics': existing_extraction.get('sub_topics', '')        # ë³´ì¡´
        }

    def _parse_ai_response_for_composition(self, response: str) -> List[Dict[str, str]]:
        """
        AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹± (engines_v5.py íŒ¨í„´)
        """
        node_sections = []
        
        # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ê°œì„ ëœ ë¶„í•  ë°©ì‹ (êµ¬ì„±ë…¸ë“œ{ìˆ«ì}: íŒ¨í„´)
        sections = []
        
        # êµ¬ì„±ë…¸ë“œ1:, êµ¬ì„±ë…¸ë“œ2:, êµ¬ì„±ë…¸ë“œ3:, êµ¬ì„±ë…¸ë“œ4: íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        pattern = r'êµ¬ì„±ë…¸ë“œ\d+:'
        parts = re.split(pattern, response)
        
        if len(parts) > 1:
            # ì²« ë²ˆì§¸ ë¶€ë¶„ì€ êµ¬ì„±ë…¸ë“œ ì´ì „ì˜ ë‚´ìš©ì´ë¯€ë¡œ ì œì™¸
            sections = parts[1:]  # êµ¬ì„±ë…¸ë“œ ë‚´ìš©ë§Œ ì¶”ì¶œ
        
        # íŒŒì‹± ë””ë²„ê¹…
        self.logger.info(f"ğŸ” AI ì‘ë‹µ íŒŒì‹± ë””ë²„ê¹…:")
        self.logger.info(f"  - ì „ì²´ ì„¹ì…˜ ìˆ˜: {len(sections)}")
        for i, section in enumerate(sections):
            self.logger.info(f"  - ì„¹ì…˜ {i}: ê¸¸ì´={len(section)}, ì‹œì‘={repr(section[:50])}...")
        
        # êµ¬ì„±ë…¸ë“œ ë‚´ìš©ë§Œ ì²˜ë¦¬ (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ë¶„í• ë¨)
        for i, section in enumerate(sections, 1):
            if not section.strip():
                self.logger.warning(f"  - êµ¬ì„±ë…¸ë“œ {i}: ë¹ˆ ì„¹ì…˜ ìŠ¤í‚µ")
                continue
                
            # ê° ì„¹ì…˜ì—ì„œ í•µì‹¬ 3ê°œ ì„¹ì…˜ ì¶”ì¶œ
            parsed_sections = parse_extraction_response(section)
            if parsed_sections:
                node_sections.append(parsed_sections)
                self.logger.info(f"  - êµ¬ì„±ë…¸ë“œ {i}: íŒŒì‹± ì„±ê³µ (í‚¤: {list(parsed_sections.keys())})")
            else:
                self.logger.warning(f"  - êµ¬ì„±ë…¸ë“œ {i}: íŒŒì‹± ì‹¤íŒ¨")
        
        self.logger.info(f"ğŸ” ìµœì¢… íŒŒì‹± ê²°ê³¼: {len(node_sections)}ê°œ ë…¸ë“œ")
        return node_sections

    async def process_single_document(self, doc: Dict, user_output_path: str) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ - ë¬¸ì„œ ëª…ì„¸ì„œì˜ í†µí•© ë¡œì§ êµ¬í˜„
        
        Args:
            doc: ë¬¸ì„œ ì •ë³´ (title, level, composition_files, content_section ë“±)
            user_output_path: ì‚¬ìš©ì ì§€ì • ì €ì¥ ê²½ë¡œ
            
        Returns:
            Dict: {output: {...}, error: str|None}
            
        ì²˜ë¦¬ ê³¼ì •:
        1. ëª¨ë“  ë…¸ë“œ: ì¶”ì¶œ ì‘ì—… ìˆ˜í–‰ (generate_extract_section)
        2. ëª¨ë“  ë…¸ë“œ: ì¶”ì¶œ ê²°ê³¼ ì €ì¥ (save_extraction_result)
        3. ë¹„ë¦¬í”„ ë…¸ë“œë§Œ: ì—…ë°ì´íŠ¸ ê³¼ì • 
           - update_current_extraction_section
           - update_composition_extraction_sections
        """
        try:
            doc_title = doc.get('title', 'Unknown')
            is_non_leaf = len(doc.get('composition_files', [])) > 0
            
            self.logger.info(f"ğŸ”„ ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {doc_title} ({'ë¹„ë¦¬í”„' if is_non_leaf else 'ë¦¬í”„'})")
            
            # 1ë‹¨ê³„: ëª¨ë“  ë…¸ë“œì—ì„œ ì¶”ì¶œ ì‘ì—… ìˆ˜í–‰
            self.logger.info(f"ğŸ¤– ì¶”ì¶œ ì‘ì—… ì‹œì‘: {doc_title}")
            extraction_result = await self.generate_extract_section(doc)
            
            if not extraction_result:
                self.logger.warning(f"âš ï¸ ì¶”ì¶œ ì‹¤íŒ¨: {doc_title}")
                return {
                    'output': {},
                    'error': f"ì¶”ì¶œ ì‹¤íŒ¨: {doc_title}"
                }
            
            self.logger.info(f"âœ… ì¶”ì¶œ ì„±ê³µ: {doc_title} ({len(extraction_result)} ì„¹ì…˜)")
            
            # 2ë‹¨ê³„: ëª¨ë“  ë…¸ë“œì—ì„œ ì¶”ì¶œ ê²°ê³¼ ì €ì¥ (ê³µí†µ)
            self.logger.info(f"ğŸ’¾ ì¶”ì¶œ ê²°ê³¼ ì €ì¥ ì‹œì‘: {doc_title}")
            saved_file_path = await self.save_extraction_result(
                doc=doc,
                extraction_result=extraction_result,
                user_output_path=user_output_path
            )
            
            if not saved_file_path:
                self.logger.warning(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {doc_title}")
                return {
                    'output': {},
                    'error': f"ì €ì¥ ì‹¤íŒ¨: {doc_title}"
                }
            
            self.logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {doc_title}")
            
            # 3ë‹¨ê³„: ë¹„ë¦¬í”„ ë…¸ë“œë§Œ ì—…ë°ì´íŠ¸ ê³¼ì • ì§„í–‰
            if is_non_leaf:
                self.logger.info(f"ğŸ”„ ë¹„ë¦¬í”„ ë…¸ë“œ ì—…ë°ì´íŠ¸ ì‹œì‘: {doc_title}")
                
                # í˜„ì¬ ë…¸ë“œ ì—…ë°ì´íŠ¸ (íŒŒì¼ì—ì„œ ì½ì–´ì„œ ì²˜ë¦¬)
                updated_extraction, used_composition = await self.update_current_extraction_section(
                    doc=doc,
                    user_output_path=user_output_path
                )
                
                self.logger.info(f"âœ… í˜„ì¬ ë…¸ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {doc_title}")
                
                # êµ¬ì„± ë…¸ë“œë“¤ ì—…ë°ì´íŠ¸ (íŒŒì¼ì—ì„œ ì½ì–´ì„œ ì²˜ë¦¬)
                await self.update_composition_extraction_sections(
                    parent_doc=doc,
                    parent_extraction=updated_extraction,
                    used_composition_extractions=used_composition,
                    composition_files=doc.get('composition_files', []),
                    user_output_path=user_output_path
                )
                
                self.logger.info(f"âœ… êµ¬ì„± ë…¸ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {doc_title}")
            else:
                self.logger.info(f"â­ï¸ ë¦¬í”„ ë…¸ë“œë¡œ ì—…ë°ì´íŠ¸ ë‹¨ê³„ ê±´ë„ˆëœ€: {doc_title}")
            
            # ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            self.logger.info(f"ğŸ‰ ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {doc_title}")
            return {
                'output': {
                    'doc_title': doc_title,
                    'node_type': 'non_leaf' if is_non_leaf else 'leaf',
                    'composition_files_count': len(doc.get('composition_files', []))
                },
                'error': None
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {doc.get('title', 'Unknown')} - {e}")
            return {
                'output': {},
                'error': f"ì²˜ë¦¬ ì‹¤íŒ¨: {doc.get('title', 'Unknown')} - {str(e)}"
            }

