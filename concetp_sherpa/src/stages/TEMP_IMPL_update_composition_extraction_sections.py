# ìƒì„± ì‹œê°„: Sat Sep 13 12:42:32 KST 2025
# í•µì‹¬ ë‚´ìš©: update_composition_extraction_sections TDD êµ¬í˜„ - engines_v5.py ì¼ê´„ ì—…ë°ì´íŠ¸ íŒ¨í„´ ì ìš©
# ìƒì„¸ ë‚´ìš©:
#   - TempContentProcessingImpl (ë¼ì¸ 30-75): ë©”ì¸ í´ë˜ìŠ¤ ë° ì´ˆê¸°í™”
#   - update_composition_extraction_sections (ë¼ì¸ 77-100): ë©”ì¸ í•¨ìˆ˜ - ê°„ì†Œí™”ëœ í”Œë¡œìš°
#   - _update_all_composition_sections (ë¼ì¸ 102-140): AI ì¼ê´„ í˜¸ì¶œ (engines_v5.py íŒ¨í„´)
#   - _parse_and_update_all_composition_nodes (ë¼ì¸ 142-200): AI ì‘ë‹µ íŒŒì‹± ë° ê°œë³„ ì €ì¥
#   - _get_composition_file_path (ë¼ì¸ 202-215): êµ¬ì„± íŒŒì¼ ê²½ë¡œ êµ¬ì„± (ê¸°ì¡´ íŒ¨í„´)
#   - _load_existing_extraction (ë¼ì¸ 217-240): ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ë¡œë“œ (ì£¼ìš”/ë¶€ì°¨ í™”ì œ ë³´ì¡´ìš©)
#   - _merge_with_preserved_topics (ë¼ì¸ 242-265): ì—…ë°ì´íŠ¸ëœ ì„¹ì…˜ê³¼ ê¸°ì¡´ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ê²°í•©
#   - _parse_ai_response (ë¼ì¸ 267-300): AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹±
#   - _parse_extraction_section (ë¼ì¸ 302-325): ì¶”ì¶œ ì„¹ì…˜ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
#   - _save_updated_extraction_to_file (ë¼ì¸ 327-350): ê¸°ì¡´ ì €ì¥ ë¡œì§ ì¬í™œìš©
# ìƒíƒœ: active

import os
import re
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

# ì‹¤ì œ êµ¬í˜„ëœ ëª¨ë“ˆ í™œìš©
import sys
refactoring_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, refactoring_root)
sys.path.append('/home/nadle/projects/Knowledge_Sherpa/v2/development/book_pipeline_refactored/src')

from src.utils.text_utils import normalize_title
from src.services.ai_service_v4 import AIService

# content_processing ì „ìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ - ì§ì ‘ êµ¬í˜„ (ì„í¬íŠ¸ ì´ìŠˆ íšŒí”¼)
def combine_extraction_sections(extraction_result: Dict[str, str]) -> str:
    """ì¶”ì¶œ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    if not extraction_result:
        return ""
    
    formatted_parts = []
    
    # ì„¹ì…˜ ìˆœì„œëŒ€ë¡œ í¬ë§·íŒ…
    section_keys = ['core_content', 'detailed_core_content', 'detailed_content', 'main_topics', 'sub_topics']
    
    for key in section_keys:
        if key in extraction_result and extraction_result[key].strip():
            formatted_parts.append(extraction_result[key])
            formatted_parts.append("")  # ì„¹ì…˜ ê°„ ë¹ˆ ì¤„
    
    return "\n".join(formatted_parts)

def update_extraction_section(file_path: str, formatted_content: str) -> bool:
    """íŒŒì¼ì˜ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸"""
    if not formatted_content:
        print(f"âš ï¸ ì—…ë°ì´íŠ¸í•  ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {file_path}")
        return False
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ì¶”ì¶œ ì„¹ì…˜ íŒ¨í„´ ì°¾ê¸°
        extraction_pattern = r'(# ì¶”ì¶œ\n---\n)(.*?)(?=\n# ë‚´ìš©|$)'
        
        if re.search(extraction_pattern, content, re.DOTALL):
            # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸
            new_content = re.sub(
                extraction_pattern,
                f'\\1{formatted_content}\n',
                content,
                flags=re.DOTALL
            )
        else:
            # ì¶”ì¶œ ì„¹ì…˜ì´ ì—†ìœ¼ë©´ # ë‚´ìš© ì•ì— ì¶”ê°€
            content_pattern = r'(\n# ë‚´ìš©)'
            if re.search(content_pattern, content):
                new_content = re.sub(
                    content_pattern,
                    f'\n# ì¶”ì¶œ\n---\n{formatted_content}\n\\1',
                    content
                )
            else:
                # # ë‚´ìš© ì„¹ì…˜ë„ ì—†ìœ¼ë©´ ëì— ì¶”ê°€
                new_content = content + f'\n\n# ì¶”ì¶œ\n---\n{formatted_content}\n'
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"âœ… ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {file_path} - {e}")
        return False


class TempContentProcessingImpl:
    """TEMP êµ¬í˜„: update_composition_extraction_sections - engines_v5.py ì¼ê´„ ì—…ë°ì´íŠ¸ íŒ¨í„´"""
    
    def __init__(self, config: Dict, ai_service: AIService):
        self.config = config
        self.ai_service = ai_service
        self.api_calls_counter = 0
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

    async def update_composition_extraction_sections(self, 
                                                   parent_doc: Dict,
                                                   parent_extraction: Dict,
                                                   used_composition_extractions: str,
                                                   composition_files: List[str],
                                                   user_output_path: str) -> None:
        """
        êµ¬ì„± ë…¸ë“œë“¤ì˜ ì¶”ì¶œ ì„¹ì…˜ì„ ë¶€ëª¨ ë…¸ë“œ ì—…ë°ì´íŠ¸ ë‚´ìš© ë°˜ì˜í•˜ì—¬ ì¼ê´„ ì—…ë°ì´íŠ¸
        engines_v5.py íŒ¨í„´: í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ëª¨ë“  êµ¬ì„± ë…¸ë“œ ì—…ë°ì´íŠ¸
        
        Args:
            parent_doc: ë¶€ëª¨ ë…¸ë“œ ë¬¸ì„œ (filename ì •ë³´ í¬í•¨)
            parent_extraction: ì—…ë°ì´íŠ¸ëœ ë¶€ëª¨ ë…¸ë“œ ì¶”ì¶œ ì„¹ì…˜ 
            used_composition_extractions: ì‚¬ìš©ëœ êµ¬ì„± ë…¸ë“œë“¤ì˜ ê²°í•©ëœ ì¶”ì¶œ ì„¹ì…˜
            composition_files: êµ¬ì„± íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
            user_output_path: ì‚¬ìš©ì ì§€ì • ì €ì¥ ê²½ë¡œ
        """
        self.logger.info(f"ğŸ”„ êµ¬ì„± ë…¸ë“œ ì¼ê´„ ì—…ë°ì´íŠ¸ ì‹œì‘")
        
        if not composition_files:
            self.logger.info("êµ¬ì„± íŒŒì¼ì´ ì—†ì–´ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            return
            
        self.logger.info(f"ğŸ“ ì²˜ë¦¬í•  êµ¬ì„± íŒŒì¼ ìˆ˜: {len(composition_files)}ê°œ")
        
        try:
            # 1ë‹¨ê³„: í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ëª¨ë“  êµ¬ì„± ë…¸ë“œ ì—…ë°ì´íŠ¸
            response = await self._update_all_composition_sections(
                parent_extraction=parent_extraction,
                used_composition_extractions=used_composition_extractions,
                composition_files=composition_files
            )
            
            # 2ë‹¨ê³„: AI ì‘ë‹µ íŒŒì‹±
            node_sections = await self._parse_ai_response_to_node_sections(response)
            
            # 3ë‹¨ê³„: ê° êµ¬ì„± ë…¸ë“œ ê°œë³„ ì €ì¥
            await self.save_each_composition_node(
                node_sections=node_sections,
                parent_doc=parent_doc,
                composition_files=composition_files,
                user_output_path=user_output_path
            )
            
            self.logger.info(f"ğŸ‰ êµ¬ì„± ë…¸ë“œ ì¼ê´„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(composition_files)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì„± ë…¸ë“œ ì¼ê´„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    async def _update_all_composition_sections(self, 
                                             parent_extraction: Dict,
                                             used_composition_extractions: str,
                                             composition_files: List[str]) -> str:
        """
        engines_v5.py íŒ¨í„´: í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ëª¨ë“  êµ¬ì„± ë…¸ë“œì˜ í•µì‹¬ 3ê°œ ì„¹ì…˜ ì—…ë°ì´íŠ¸
        """
        # ë¶€ëª¨ ë…¸ë“œì˜ í•µì‹¬ 3ê°œ ì„¹ì…˜ë§Œ ì¶”ì¶œ (engines_v5.py ë™ì¼)
        parent_core = parent_extraction.get('core_content', '').replace('## í•µì‹¬ ë‚´ìš©', '').strip()
        parent_detailed_core = parent_extraction.get('detailed_core_content', '').replace('## ìƒì„¸ í•µì‹¬ ë‚´ìš©', '').strip()
        parent_detailed_info = parent_extraction.get('detailed_content', '').replace('## ìƒì„¸ ì •ë³´', '').strip()
        
        # êµ¬ì„± íŒŒì¼ ìˆ˜ ì •ë³´ ì¶”ê°€ë¡œ AI ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
        composition_count = len(composition_files)
        
        # engines_v5.pyì—ì„œ ìˆ˜ì • ìš”ì²­ëœ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ - êµ¬ì„± íŒŒì¼ ìˆ˜ ëª…ì‹œ
        prompt = f"""ë‹¤ìŒì€ ë¶€ëª¨ ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì´ {composition_count}ê°œ** êµ¬ì„± ë…¸ë“œë“¤ì˜ í•µì‹¬ 3ê°€ì§€ ì •ë³´ ì„¹ì…˜ë§Œ ê°œì„ í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.

**ë¶€ëª¨ ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©:**
í•µì‹¬ ë‚´ìš©: {parent_core}
ìƒì„¸ í•µì‹¬ ë‚´ìš©: {parent_detailed_core}
ìƒì„¸ ì •ë³´: {parent_detailed_info}

**êµ¬ì„± ë…¸ë“œë“¤ì˜ í˜„ì¬ ë‚´ìš©:**
{used_composition_extractions}

ë¶€ëª¨ ë…¸ë“œì˜ ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ê° êµ¬ì„± ë…¸ë“œì˜ **3ê°€ì§€ ì •ë³´ ì„¹ì…˜(í•µì‹¬ ë‚´ìš©, ìƒì„¸ í•µì‹¬ ë‚´ìš©, ìƒì„¸ ì •ë³´)ë§Œ** ê°œì„ í•´ì£¼ì„¸ìš”.
ê° êµ¬ì„± ë…¸ë“œì˜ ê³ ìœ í•œ íŠ¹ì„±ì€ ìœ ì§€í•˜ë˜, ë¶€ëª¨ì™€ì˜ ì¼ê´€ì„±ê³¼ ì—°ê²°ì„±ì„ ë°˜ì˜í•´ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ {composition_count}ê°œ ëª¨ë“  êµ¬ì„±ë…¸ë“œì— ëŒ€í•´ ì‘ë‹µí•´ì£¼ì„¸ìš”.**

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

êµ¬ì„±ë…¸ë“œ1:
## í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ ìƒì„¸ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ ì •ë³´
[ê°œì„ ëœ ìƒì„¸ ì •ë³´]

êµ¬ì„±ë…¸ë“œ2:
## í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ í•µì‹¬ ë‚´ìš©
[ê°œì„ ëœ ìƒì„¸ í•µì‹¬ ë‚´ìš©]

## ìƒì„¸ ì •ë³´
[ê°œì„ ëœ ìƒì„¸ ì •ë³´]

**ì¤‘ìš”**: ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ "## " (í•´ì‹œ 2ê°œ + ê³µë°±)ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì œëª©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."""

        # ë‹¨ì¼ AI í˜¸ì¶œ
        response = await self.ai_service.query_single_request(prompt)
        self.api_calls_counter += 1
        self.logger.info(f"âœ… AI ì¼ê´„ í˜¸ì¶œ ì™„ë£Œ (í˜¸ì¶œ íšŸìˆ˜: {self.api_calls_counter})")
        
        return response

    async def _parse_ai_response_to_node_sections(self, response: str) -> List[Dict[str, str]]:
        """
        AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹± (SRP: íŒŒì‹±ë§Œ ë‹´ë‹¹)
        """
        try:
            # AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹±
            node_sections = self._parse_ai_response(response)
            
            self.logger.info(f"ğŸ” ìµœì¢… íŒŒì‹± ê²°ê³¼: {len(node_sections)}ê°œ ë…¸ë“œ")
            return node_sections
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise

    async def save_each_composition_node(self, 
                                       node_sections: List[Dict[str, str]],
                                       parent_doc: Dict,
                                       composition_files: List[str],
                                       user_output_path: str) -> None:
        """
        íŒŒì‹±ëœ ë…¸ë“œ ì„¹ì…˜ë“¤ì„ ê°ê° ê°œë³„ ì €ì¥ (SRP: ì €ì¥ë§Œ ë‹´ë‹¹)
        """
        try:
            # ğŸ” ê²€ì¦: íŒŒì‹±ëœ ì„¹ì…˜ ìˆ˜ì™€ êµ¬ì„± íŒŒì¼ ìˆ˜ ë¹„êµ
            expected_count = len(composition_files)
            parsed_count = len(node_sections)
            
            self.logger.info(f"ğŸ“Š AI ì‘ë‹µ íŒŒì‹± ê²°ê³¼: ì˜ˆìƒ {expected_count}ê°œ, íŒŒì‹± {parsed_count}ê°œ")
            
            if parsed_count < expected_count:
                self.logger.warning(f"âš ï¸ AI ì‘ë‹µì—ì„œ {expected_count - parsed_count}ê°œ êµ¬ì„±ë…¸ë“œ ì„¹ì…˜ ëˆ„ë½")
            elif parsed_count == expected_count:
                self.logger.info("âœ… AI ì‘ë‹µ íŒŒì‹± ì™„ë£Œ: ëª¨ë“  ì„¹ì…˜ ì •ìƒ")
            
            
            successful_updates = 0
            
            # ê° êµ¬ì„± íŒŒì¼ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
            for i, comp_file in enumerate(composition_files):
                try:
                    if i >= len(node_sections):
                        self.logger.warning(f"âš ï¸ AI ì‘ë‹µì—ì„œ {comp_file}ì— í•´ë‹¹í•˜ëŠ” ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        continue
                    
                    # íŒŒì¼ ê²½ë¡œ êµ¬ì„±
                    comp_file_path = self._get_composition_file_path(comp_file, user_output_path, parent_doc)
                    
                    # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ ë¡œë“œ (ì£¼ìš”/ë¶€ì°¨ í™”ì œ ë³´ì¡´ìš©)
                    existing_extraction = await self._load_parent_topic_extractions(comp_file_path)
                    
                    # ì—…ë°ì´íŠ¸ëœ ì„¹ì…˜ê³¼ ê¸°ì¡´ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ê²°í•©
                    final_sections = self._merge_with_preserved_topics(node_sections[i], existing_extraction)
                    
                    # ê°œë³„ ì €ì¥ (ê¸°ì¡´ ë¡œì§ ì¬í™œìš©)
                    await self._save_updated_extraction_to_file(
                        file_path=comp_file_path,
                        updated_extraction=final_sections,
                        status_marker="<ë¶€ëª¨ ë…¸ë“œ ë°˜ì˜ ì™„ë£Œ>"
                    )
                    
                    successful_updates += 1
                    self.logger.info(f"âœ… êµ¬ì„± ë…¸ë“œ ì €ì¥ ì™„ë£Œ: {comp_file}")
                    
                except Exception as e:
                    self.logger.error(f"âŒ êµ¬ì„± ë…¸ë“œ {comp_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            self.logger.info(f"ğŸ“Š êµ¬ì„± ë…¸ë“œ ê°œë³„ ì €ì¥ ì™„ë£Œ: {successful_updates}/{len(composition_files)}ê°œ")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ë° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def _get_composition_file_path(self, comp_file: str, user_output_path: str, parent_doc: Dict) -> Path:
        """
        êµ¬ì„± íŒŒì¼ ê²½ë¡œ êµ¬ì„± (parent_doc file_nameì—ì„œ ê²½ë¡œ ì¶”ì¶œ)
        parent_docì˜ file_nameì—ì„œ {ì±…í´ë”}/{ì¥í´ë”}/unified_info_docs ê²½ë¡œë¥¼ ì¶”ì¶œí•˜ì—¬ í™œìš©
        """
        # parent_docì˜ file_nameì—ì„œ ê²½ë¡œ ì •ë³´ ì¶”ì¶œ
        parent_filename = parent_doc.get('file_name', '')
        if not parent_filename:
            raise ValueError("parent_docì— file_nameì´ ì—†ìŠµë‹ˆë‹¤")
        
        # file_nameì—ì„œ ë””ë ‰í† ë¦¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ (íŒŒì¼ëª… ì œì™¸)
        # ì˜ˆ: "Data_Oriented_Programming/1_Complexity_of_object_oriented_programming/unified_info_docs/16_lev2_1.1_OOP_design_Classic_or_classical_info.md"
        # -> "Data_Oriented_Programming/1_Complexity_of_object_oriented_programming/unified_info_docs"
        parent_dir = '/'.join(parent_filename.split('/')[:-1])
        
        comp_file_path = Path(user_output_path) / parent_dir / comp_file
        return comp_file_path

    async def _load_parent_topic_extractions(self, comp_file_path: Path) -> Dict[str, str]:
        """ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ì—ì„œ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ë¡œë“œ (ë³´ì¡´ìš©)"""
        try:
            if not comp_file_path.exists():
                raise FileNotFoundError(f"êµ¬ì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {comp_file_path}")
            
            with open(comp_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì¶”ì¶œ ì„¹ì…˜ ì¶”ì¶œ
            extraction_match = re.search(r'# ì¶”ì¶œ\n---\n(.*?)(?=\n# |$)', content, re.DOTALL)
            if not extraction_match:
                raise ValueError(f"ì¶”ì¶œ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {comp_file_path}")
                
            extraction_content = extraction_match.group(1).strip()
            if not extraction_content:
                raise ValueError(f"ì¶”ì¶œ ì„¹ì…˜ì´ ë¹„ì–´ìˆìŒ: {comp_file_path}")
                
            parsed_sections = self._parse_extraction_section(extraction_content)
            if not parsed_sections:
                raise ValueError(f"ì¶”ì¶œ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨: {comp_file_path}")
                
            return parsed_sections
            
        except Exception as e:
            self.logger.error(f"âŒ ë¶€ëª¨ í™”ì œ ì¶”ì¶œ ì‹¤íŒ¨: {comp_file_path} - {e}")
            raise RuntimeError(f"ë¶€ëª¨ í™”ì œ ì¶”ì¶œ ì‹¤íŒ¨: {comp_file_path} - {e}")

    def _merge_with_preserved_topics(self, updated_sections: Dict[str, str], existing_extraction: Dict[str, str]) -> Dict[str, str]:
        """
        ì—…ë°ì´íŠ¸ëœ í•µì‹¬ 3ê°œ ì„¹ì…˜ê³¼ ê¸°ì¡´ ì£¼ìš”/ë¶€ì°¨ í™”ì œ ê²°í•©
        engines_v5.py ë³´ì¡´ ë¡œì§
        """
        return {
            'core_content': updated_sections.get('core_content', existing_extraction.get('core_content', '')),
            'detailed_core_content': updated_sections.get('detailed_core_content', existing_extraction.get('detailed_core_content', '')),
            'detailed_content': updated_sections.get('detailed_content', existing_extraction.get('detailed_content', '')),
            'main_topics': existing_extraction.get('main_topics', ''),      # ğŸ”¥ ë³´ì¡´
            'sub_topics': existing_extraction.get('sub_topics', '')        # ğŸ”¥ ë³´ì¡´
        }

    def _parse_ai_response(self, response: str) -> List[Dict[str, str]]:
        """
        AI ì‘ë‹µì„ êµ¬ì„± ë…¸ë“œë³„ë¡œ íŒŒì‹± (engines_v5.py íŒ¨í„´)
        """
        node_sections = []
        
        # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ê°œì„ ëœ ë¶„í•  ë°©ì‹ (êµ¬ì„±ë…¸ë“œ{ìˆ«ì}: íŒ¨í„´)
        import re
        sections = []
        
        # êµ¬ì„±ë…¸ë“œ1:, êµ¬ì„±ë…¸ë“œ2:, êµ¬ì„±ë…¸ë“œ3:, êµ¬ì„±ë…¸ë“œ4: íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        pattern = r'êµ¬ì„±ë…¸ë“œ\d+:'
        parts = re.split(pattern, response)
        
        if len(parts) > 1:
            # ì²« ë²ˆì§¸ ë¶€ë¶„ì€ êµ¬ì„±ë…¸ë“œ ì´ì „ì˜ ë‚´ìš©ì´ë¯€ë¡œ ì œì™¸
            sections = parts[1:]  # êµ¬ì„±ë…¸ë“œ ë‚´ìš©ë§Œ ì¶”ì¶œ
        
        # ğŸ” íŒŒì‹± ë””ë²„ê¹…
        self.logger.info(f"ğŸ” AI ì‘ë‹µ íŒŒì‹± ë””ë²„ê¹…:")
        self.logger.info(f"  - ì „ì²´ ì„¹ì…˜ ìˆ˜: {len(sections)}")
        for i, section in enumerate(sections):
            self.logger.info(f"  - ì„¹ì…˜ {i}: ê¸¸ì´={len(section)}, ì‹œì‘={repr(section[:50])}...")
        
        # êµ¬ì„±ë…¸ë“œ ë‚´ìš©ë§Œ ì²˜ë¦¬ (ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ë¶„í• ë¨)
        for i, section in enumerate(sections, 1):
            if not section.strip():
                self.logger.warning(f"  - êµ¬ì„±ë…¸ë“œ {i}: ë¹ˆ ì„¹ì…˜ ìŠ¤í‚µ")
                continue
                
            # ê° ì„¹ì…˜ì—ì„œ í•µì‹¬ 3ê°œ ì„¹ì…˜ ì¶”ì¶œ
            parsed_sections = self._parse_extraction_section(section)
            if parsed_sections:
                node_sections.append(parsed_sections)
                self.logger.info(f"  - êµ¬ì„±ë…¸ë“œ {i}: íŒŒì‹± ì„±ê³µ (í‚¤: {list(parsed_sections.keys())})")
            else:
                self.logger.warning(f"  - êµ¬ì„±ë…¸ë“œ {i}: íŒŒì‹± ì‹¤íŒ¨")
        
        self.logger.info(f"ğŸ” ìµœì¢… íŒŒì‹± ê²°ê³¼: {len(node_sections)}ê°œ ë…¸ë“œ")
        return node_sections

    def _parse_extraction_section(self, extraction_content: str) -> Dict[str, str]:
        """ì¶”ì¶œ ì„¹ì…˜ íŒŒì‹± ìœ í‹¸ë¦¬í‹°"""
        sections = {}
        
        # ê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ
        patterns = {
            'core_content': r'## í•µì‹¬ ë‚´ìš©\n(.*?)(?=\n## |$)',
            'detailed_core_content': r'## ìƒì„¸ í•µì‹¬ ë‚´ìš©\n(.*?)(?=\n## |$)',
            'detailed_content': r'## ìƒì„¸ ì •ë³´\n(.*?)(?=\n## |$)',
            'main_topics': r'## ì£¼ìš” í™”ì œ\n(.*?)(?=\n## |$)',
            'sub_topics': r'## ë¶€ì°¨ í™”ì œ\n(.*?)(?=\n## |$)'
        }
        
        for section_key, pattern in patterns.items():
            match = re.search(pattern, extraction_content, re.DOTALL)
            if match:
                section_title = section_key.replace('_', ' ').replace('content', 'ë‚´ìš©').replace('detailed core', 'ìƒì„¸ í•µì‹¬').replace('detailed', 'ìƒì„¸ ì •ë³´').replace('main topics', 'ì£¼ìš” í™”ì œ').replace('sub topics', 'ë¶€ì°¨ í™”ì œ')
                sections[section_key] = f"## {section_title.title()}\n{match.group(1).strip()}"
        
        return sections

    async def _save_updated_extraction_to_file(self, file_path: Path, updated_extraction: Dict, status_marker: str):
        """
        ê¸°ì¡´ ContentProcessingStage._save_updated_extraction_to_file ë¡œì§ ì¬í™œìš©
        """
        try:
            # ìƒˆë¡œìš´ ì¶”ì¶œ ì„¹ì…˜ ë‚´ìš© í¬ë§·íŒ… - ìƒíƒœ ë§ˆí‚¹ í¬í•¨
            formatted_extraction = combine_extraction_sections(updated_extraction)
            # ìƒíƒœ ë§ˆí‚¹ì„ ì¶”ì¶œ ì„¹ì…˜ ë§¨ ì•ì— ì¶”ê°€
            formatted_extraction = f"{status_marker}\n\n{formatted_extraction}"
            
            # ê¸°ì¡´ ì¶”ì¶œ ì„¹ì…˜ êµì²´ (update_file_extraction_sectionì€ boolean ë°˜í™˜)
            success = update_extraction_section(str(file_path), formatted_extraction)
            
            if success:
                self.logger.info(f"ğŸ’¾ ì—…ë°ì´íŠ¸ëœ ì¶”ì¶œ ì„¹ì…˜ ì €ì¥ ì™„ë£Œ: {file_path.name}")
            else:
                raise Exception("ì¶”ì¶œ ì„¹ì…˜ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ê°€ ì‹¤íŒ¨ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
        except Exception as e:
            self.logger.error(f"âŒ ì—…ë°ì´íŠ¸ëœ ì¶”ì¶œ ì„¹ì…˜ ì €ì¥ ì‹¤íŒ¨: {file_path} - {e}")
            raise