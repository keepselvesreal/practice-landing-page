# ìƒì„± ì‹œê°„: Wed Sep 10 12:34:15 KST 2025
# í•µì‹¬ ë‚´ìš©: ë°ì´í„° ê¸°ë°˜ í†µí•© ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„± ë‹¨ê³„ í”„ë¡œì„¸ì„œ (ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬, íŒŒì¼ ì €ì¥ ì œê±°)
# ìƒì„¸ ë‚´ìš©:
#   - IntegratedNodeGenerationStage (ë¼ì¸ 31-200): ë©”ì¸ í†µí•© ë…¸ë“œ ìƒì„± í´ë˜ìŠ¤ (ì™„ì „ ë°ì´í„° ê¸°ë°˜)
#   - process (ë¼ì¸ 47-120): ë©”ì¸ ì²˜ë¦¬ ë¡œì§ (workspace_resultì˜ chapters_data ìˆœíšŒ)
#   - generate_node_documents (ë¼ì¸ 122-160): ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„± (file_name + content ë°˜í™˜)
#   - generate_content_documents (ë¼ì¸ 162-200): AI ê¸°ë°˜ ì½˜í…ì¸  ë¬¸ì„œ ìƒì„± (file_name + content ë°˜í™˜)  
#   - integrate_documents (ë¼ì¸ 202-240): í†µí•© ë¬¸ì„œ ìƒì„± (file_name + content ë°˜í™˜)
# ìƒíƒœ: active
# ì°¸ì¡°: integrated_node_generation_stage_v3.py (ë°ì´í„° ê¸°ë°˜ ì²˜ë¦¬ë¡œ ì™„ì „ ê°œí¸)

import sys
from pathlib import Path
from typing import Dict, Any, List

# ê¸°ë³¸ í´ë˜ìŠ¤ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent))
from core.base.base_processor import BaseProcessor

# ì„œë¹„ìŠ¤ ì„í¬íŠ¸  
from services.content_document_service_v4 import ContentDocumentService
from services.node_document_service_v2 import NodeDocumentService
# í†µí•© ë¡œê±° ì„í¬íŠ¸
from utils.logger_v2 import Logger
# í…ìŠ¤íŠ¸ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils.text_utils import normalize_title

class IntegratedNodeGenerationStage(BaseProcessor):
    """ë°ì´í„° ê¸°ë°˜ í†µí•© ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„± ë‹¨ê³„ í”„ë¡œì„¸ì„œ (3ë‹¨ê³„: ë…¸ë“œì •ë³´ë¬¸ì„œìƒì„± â†’ ì½˜í…ì¸ ë…¸ë“œì¶”ì¶œ â†’ ë¬¸ì„œí†µí•©)"""
    
    def __init__(self, config_manager, logger_factory=None):
        super().__init__(config_manager, logger_factory, "integrated_node_generation")
        
        # ìƒˆë¡œìš´ í†µí•© Logger ì‚¬ìš©
        self.logger = Logger(
            project_name="integrated_node_stage_v4",
            base_dir="./results",
            logs_base_dir="./logs"
        )
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.content_document_service = ContentDocumentService(config_manager, self.logger)
        self.node_document_service = NodeDocumentService(config_manager, self.logger)
        
    async def process(self, prev_stage_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë©”ì¸ í†µí•© ë…¸ë“œ ìƒì„± ì²˜ë¦¬ - workspace_resultì˜ chapters_data ê¸°ë°˜ ì²˜ë¦¬
        
        Args:
            prev_stage_result: ì´ì „ ë‹¨ê³„(workspace_preparation_stage)ì˜ ì¶œë ¥ ê²°ê³¼
        
        Returns:
            Dict: {
                'data': {
                    'book_information': Dict,             # ì±… ë©”íƒ€ë°ì´í„° ì •ë³´
                    'processed_chapters': List[Dict],  # ì²˜ë¦¬ëœ ì¥ë“¤ì˜ ì •ë³´ (chapter_title + normalized_title)
                    'unified_documents': List[Dict]    # ìƒì„±ëœ í†µí•© ë¬¸ì„œë“¤
                },
                'error': str
            }
        """
        try:
            self.logger.info("ğŸš€ **í†µí•© ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„± ë‹¨ê³„ ì‹œì‘** (ë°ì´í„° ê¸°ë°˜ ì²˜ë¦¬)")
            
            # ì´ì „ ë‹¨ê³„ ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            workspace_data = prev_stage_result.get('data', {})
            
            # ì±… ì •ë³´ ì¶”ì¶œ
            book_information = workspace_data.get('book_information', {})
            normalized_book_title = book_information.get('normalized_title', 'Unknown_Book')
            
            chapters_data = workspace_data.get('chapters_data', [])
            
            if not chapters_data:
                error_msg = "chapters_data ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
                self.logger.error(f"âŒ {error_msg}")
                return {
                    'data': {
                        'book_information': book_information,
                        'processed_chapters': [],
                        'unified_documents': []
                    },
                    'error': error_msg
                }
            
            self.logger.info(f"ğŸ“Š ì²˜ë¦¬í•  ì±•í„° ìˆ˜: {len(chapters_data)}")
            
            # ğŸ” **3ë‹¨ê³„ ìˆœì°¨ ì²˜ë¦¬**
            all_node_documents = []
            all_content_documents = []
            all_unified_documents = []
            processed_chapters = []  # ì²˜ë¦¬ëœ ì±•í„°ë“¤ ì •ë³´ ìˆ˜ì§‘
            
            # ê° ì±•í„°ë³„ë¡œ 3ë‹¨ê³„ ì²˜ë¦¬
            for i, chapter_info in enumerate(chapters_data, 1):
                chapter_title = chapter_info.get('chapter_title', f'Chapter {i}')
                self.logger.info(f"ğŸ“– **{i}/{len(chapters_data)} ì±•í„° ì²˜ë¦¬ ì‹œì‘**: {chapter_title}")
                
                # ì²˜ë¦¬ëœ ì±•í„° ì •ë³´ ìˆ˜ì§‘
                processed_chapters.append({
                    'chapter_title': chapter_title,
                    'normalized_title': normalize_title(chapter_title)
                })
                
                # 1ë‹¨ê³„: ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„±
                self.logger.info(f"ğŸ“ **1ë‹¨ê³„**: ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„± - {chapter_title}")
                node_documents = await self.generate_node_documents(chapter_info, normalized_book_title)
                all_node_documents.extend(node_documents)
                self.logger.info(f"âœ… ë…¸ë“œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {len(node_documents)}ê°œ")
                
                # 2ë‹¨ê³„: ì½˜í…ì¸  ë¬¸ì„œ ìƒì„±  
                self.logger.info(f"ğŸ” **2ë‹¨ê³„**: ì½˜í…ì¸  ë¬¸ì„œ ìƒì„± - {chapter_title}")
                content_documents = await self.generate_content_documents(chapter_info, normalized_book_title)
                all_content_documents.extend(content_documents)
                self.logger.info(f"âœ… ì½˜í…ì¸  ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {len(content_documents)}ê°œ")
                
                # 3ë‹¨ê³„: ë¬¸ì„œ í†µí•©
                self.logger.info(f"ğŸ”— **3ë‹¨ê³„**: ë¬¸ì„œ í†µí•© - {chapter_title}")
                unified_documents = await self.integrate_documents(chapter_info, node_documents, content_documents, normalized_book_title)
                all_unified_documents.extend(unified_documents)
                self.logger.info(f"âœ… ë¬¸ì„œ í†µí•© ì™„ë£Œ: {len(unified_documents)}ê°œ")
            
            # ğŸ“Š **ìµœì¢… ê²°ê³¼**
            self.logger.info(f"ğŸ‰ **í†µí•© ë…¸ë“œ ìƒì„± ì™„ë£Œ**")
            self.logger.info(f"   - ë…¸ë“œ ë¬¸ì„œ: {len(all_node_documents)}ê°œ")
            self.logger.info(f"   - ì½˜í…ì¸  ë¬¸ì„œ: {len(all_content_documents)}ê°œ")
            self.logger.info(f"   - í†µí•© ë¬¸ì„œ: {len(all_unified_documents)}ê°œ")
            
            return {
                'data': {
                    'book_information': book_information,
                    'processed_chapters': processed_chapters,
                    'unified_documents': all_unified_documents
                },
                'error': None
            }
            
        except Exception as e:
            error_msg = f"í†µí•© ë…¸ë“œ ìƒì„± ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            return {
                'data': {
                    'book_information': {},
                    'processed_chapters': [],
                    'unified_documents': []
                },
                'error': error_msg
            }
    
    async def generate_node_documents(self, chapter_info: Dict[str, Any], normalized_book_title: str) -> List[Dict[str, str]]:
        """
        1ë‹¨ê³„: ë…¸ë“œ ì •ë³´ ë¬¸ì„œ ìƒì„± (NodeDocumentService ì‚¬ìš©)
        
        Args:
            chapter_info: {
                'chapter_title': str,
                'chapter_toc': List[Dict],
                'content_text': str
            }
            normalized_book_title: ì •ê·œí™”ëœ ì±… ì œëª©
        
        Returns:
            List[Dict]: [{'file_name': str, 'content': str}, ...]
        """
        try:
            # NodeDocumentServiceë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìƒì„±
            documents = self.node_document_service.generate_documents_for_chapter(chapter_info)
            
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° ë³€ê²½: {ì •ê·œí™”ëœì¥ì´ë¦„}/info_docs/
            normalized_chapter = normalize_title(chapter_info.get('chapter_title', 'Unknown'))
            
            for doc in documents:
                old_path = doc['file_name']  # "node_info_docs/filename"
                filename = old_path.replace('node_info_docs/', '')  # "filename"
                doc['file_name'] = f"{normalized_book_title}/{normalized_chapter}/info_docs/{filename}"
            
            return documents
            
        except Exception as e:
            self.logger.error(f"âŒ ë…¸ë“œ ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def generate_content_documents(self, chapter_info: Dict[str, Any], normalized_book_title: str) -> List[Dict[str, str]]:
        """
        2ë‹¨ê³„: AI ê¸°ë°˜ ì½˜í…ì¸  ë¬¸ì„œ ìƒì„± (ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬)
        
        Args:
            chapter_info: {
                'chapter_title': str,
                'chapter_toc': List[Dict],
                'content_text': str
            }
            normalized_book_title: ì •ê·œí™”ëœ ì±… ì œëª©
        
        Returns:
            List[Dict]: [{'file_name': str, 'content': str}, ...]
        """
        try:
            chapter_title = chapter_info.get('chapter_title', 'Unknown Chapter')
            chapter_toc = chapter_info.get('chapter_toc', [])
            content_text = chapter_info.get('content_text', '')
            
            if not chapter_toc or not content_text:
                self.logger.warning(f"âš ï¸ {chapter_title}: í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # 1ë‹¨ê³„: ì„¹ì…˜ë³„ ë‚´ìš© í¬í•¨ ì—¬ë¶€ ë¶„ì„
            sections_with_content = await self.content_document_service.detect_section_content(
                chapter_sections=chapter_toc,
                chapter_content=content_text,
                stage_name="integrated_node_generation"
            )
            
            # 2ë‹¨ê³„: ë‚´ìš©ì´ ìˆëŠ” ì„¹ì…˜ë“¤ì˜ ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ
            content_sections = [section for section in sections_with_content if section.get('has_content', False)]
            
            if content_sections:
                # AIë¡œ ëª¨ë“  ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ (ë©€í‹°í„´ ë°©ì‹)
                extraction_results = await self.content_document_service.extract_section_content(
                    content_sections=content_sections,
                    chapter_content=content_text,
                    stage_name="integrated_node_generation"
                )
                
                generated_documents = []
                # ë””ë ‰í† ë¦¬ êµ¬ì¡° ë³€ê²½: {ì •ê·œí™”ëœì¥ì´ë¦„}/sections/
                normalized_chapter = normalize_title(chapter_info.get('chapter_title', 'Unknown'))
                
                for result in extraction_results:
                    section_title = result.get('section_title', 'Unknown')
                    content = result.get('extracted_content', '')  # âœ… ì˜¬ë°”ë¥¸ í•„ë“œëª… ì‚¬ìš©
                    
                    if content:
                        # íŒŒì¼ëª… ìƒì„±
                        normalized_title = normalize_title(section_title)
                        file_name = f"{normalized_book_title}/{normalized_chapter}/sections/{normalized_title}.md"
                        
                        generated_documents.append({
                            'file_name': file_name,
                            'content': content
                        })
                
                return generated_documents
            else:
                return []
            
        except Exception as e:
            self.logger.error(f"âŒ ì½˜í…ì¸  ë¬¸ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def integrate_documents(self, chapter_info: Dict[str, Any], 
                                node_documents: List[Dict[str, str]], 
                                content_documents: List[Dict[str, str]], 
                                normalized_book_title: str) -> List[Dict[str, str]]:
        """
        3ë‹¨ê³„: ë…¸ë“œ ë¬¸ì„œì™€ ì½˜í…ì¸  ë¬¸ì„œ í†µí•© (ë…¸ë“œë³„ ë§¤ì¹­í•˜ì—¬ í†µí•©)
        
        ê° ë…¸ë“œ ì •ë³´ ë¬¸ì„œì˜ ë‚´ìš© ì„¹ì…˜ì— ëŒ€ì‘ë˜ëŠ” ì½˜í…ì¸  ë¬¸ì„œë¥¼ ì‚½ì…
        
        Args:
            chapter_info: ì±•í„° ì •ë³´
            node_documents: 1ë‹¨ê³„ ìƒì„±ëœ ë…¸ë“œ ë¬¸ì„œë“¤
            content_documents: 2ë‹¨ê³„ ìƒì„±ëœ ì½˜í…ì¸  ë¬¸ì„œë“¤
            normalized_book_title: ì •ê·œí™”ëœ ì±… ì œëª©
        
        Returns:
            List[Dict]: [{'file_name': str, 'content': str}, ...]
        """
        try:
            chapter_title = chapter_info.get('chapter_title', 'Unknown Chapter')
            chapter_toc = chapter_info.get('chapter_toc', [])
            
            if not node_documents:
                self.logger.warning(f"âš ï¸ {chapter_title}: ë…¸ë“œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ì½˜í…ì¸  ë¬¸ì„œë¥¼ ì„¹ì…˜ ì œëª©ìœ¼ë¡œ ë§¤í•‘
            content_map = {}
            for content_doc in content_documents:
                # {ì •ê·œí™”ëœì¥ì´ë¦„}/sections/{ì„¹ì…˜title}.mdì—ì„œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ
                file_path = content_doc['file_name']
                if '/sections/' in file_path and file_path.endswith('.md'):
                    # íŒŒì¼ëª…ì—ì„œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ (ë§ˆì§€ë§‰ / ì´í›„ë¶€í„° .md ì œê±°)
                    section_title = file_path.split('/')[-1][:-3]  # .md ì œê±°
                    content_map[section_title] = content_doc['content']
            
            unified_documents = []
            
            # ê° ë…¸ë“œ ë¬¸ì„œë³„ë¡œ í†µí•© ë¬¸ì„œ ìƒì„±
            for node_doc in node_documents:
                node_file_name = node_doc['file_name']
                node_content = node_doc['content']
                
                # ë…¸ë“œ ì •ë³´ ë¬¸ì„œ íŒŒì¼ëª…ì—ì„œ ë…¸ë“œ ì •ë³´ ì¶”ì¶œ
                # {ì •ê·œí™”ëœì¥ì´ë¦„}/info_docs/{filename} â†’ {filename}
                base_filename = node_file_name.split('/info_docs/')[-1]
                
                # í•´ë‹¹ ë…¸ë“œì˜ TOC ì •ë³´ ì°¾ê¸°
                corresponding_node = None
                for node in chapter_toc:
                    node_title_normalized = normalize_title(node.get('title', ''))
                    if node_title_normalized in base_filename:
                        corresponding_node = node
                        break
                
                # í†µí•© ë¬¸ì„œ ë‚´ìš© ìƒì„± - í•­ìƒ ë‚´ìš©ê³¼ êµ¬ì„± ì„¹ì…˜ ì²˜ë¦¬
                integrated_content = node_content
                
                # í•´ë‹¹ ë…¸ë“œ ì •ë³´ê°€ ìˆìœ¼ë©´ ë‚´ìš©ê³¼ êµ¬ì„± ì„¹ì…˜ ëª¨ë‘ ì²˜ë¦¬
                if corresponding_node:
                    node_title = corresponding_node.get('title', '')
                    node_level = corresponding_node.get('level', 1)
                    node_title_normalized = normalize_title(node_title)
                    
                    # ë ˆë²¨ì— ë”°ë¥¸ í—¤ë” ìƒì„±
                    header_prefix = "#" * node_level
                    content_header = f"{header_prefix} {node_title}"
                    
                    # ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í—¤ë”ë§Œ)
                    if node_title_normalized in content_map:
                        content_to_insert = f"{content_header}\n{content_map[node_title_normalized]}"
                    else:
                        content_to_insert = content_header
                    
                    # "# ë‚´ìš©" ì„¹ì…˜ê³¼ "# êµ¬ì„±" ì„¹ì…˜ ëª¨ë‘ ì²˜ë¦¬
                    lines = integrated_content.split('\n')
                    new_lines = []
                    content_section_found = False
                    config_section_found = False
                    prev_line = ""
                    
                    for i, line in enumerate(lines):
                        new_lines.append(line)
                        
                        # ê¸°ë³¸ í…œí”Œë¦¿ì˜ êµ¬ë¶„ì„  ë°”ë¡œ ì•„ë˜ì— ë‚´ìš© ì‚½ì…
                        if line.strip() == "---" and prev_line.strip() == "# ë‚´ìš©" and not content_section_found:
                            new_lines.append(content_to_insert)
                            content_section_found = True
                            
                        # ê¸°ë³¸ í…œí”Œë¦¿ì˜ êµ¬ë¶„ì„  ë°”ë¡œ ì•„ë˜ì— êµ¬ì„± ì •ë³´ ì‚½ì…
                        elif line.strip() == "---" and prev_line.strip() == "# êµ¬ì„±" and not config_section_found:
                            # ìì‹ ë…¸ë“œë“¤ì˜ ì •ë³´ ë¬¸ì„œ íŒŒì¼ëª… ì¶”ê°€
                            descendants_files = self._get_all_descendants_info(corresponding_node, chapter_toc)
                            descendants_text = "\n".join(descendants_files) if descendants_files else ""
                            new_lines.append(descendants_text)
                            config_section_found = True
                        
                        prev_line = line
                    
                    integrated_content = '\n'.join(new_lines)
                
                # í†µí•© ë¬¸ì„œë¡œ ì €ì¥ ({ì •ê·œí™”ëœì±…ì œëª©}/{ì •ê·œí™”ëœì¥ì´ë¦„}/unified_info_docs/ í´ë”ì—)
                normalized_chapter = normalize_title(chapter_info.get('chapter_title', 'Unknown'))
                unified_file_name = f"{normalized_book_title}/{normalized_chapter}/unified_info_docs/{base_filename}"
                
                unified_documents.append({
                    'file_name': unified_file_name,
                    'content': integrated_content
                })
            
            return unified_documents
            
        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ì„œ í†µí•© ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _get_all_descendants_info(self, node: Dict[str, Any], all_nodes: List[Dict[str, Any]]) -> List[str]:
        """ë…¸ë“œì˜ ëª¨ë“  í•˜ìœ„ ë…¸ë“œë“¤ì˜ ì •ë³´ ë¬¸ì„œ íŒŒì¼ëª…ì„ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        # ëª¨ë“  í•˜ìœ„ ë…¸ë“œ IDë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘
        descendant_ids = self._collect_descendant_ids(node, all_nodes, set())
        
        # IDë¡œ ì •ë ¬
        descendant_ids = sorted(descendant_ids)
        
        # ê° ë…¸ë“œ IDì— ëŒ€ì‘í•˜ëŠ” íŒŒì¼ëª… ìƒì„±
        descendant_files = []
        for node_id in descendant_ids:
            descendant_node = next((n for n in all_nodes if n.get('id') == node_id), None)
            if descendant_node:
                # íŒŒì¼ëª… ìƒì„±
                title_clean = normalize_title(descendant_node['title'])
                filename = f"{descendant_node['id']:02d}_lev{descendant_node['level']}_{title_clean}_info.md"
                descendant_files.append(filename)
        
        return descendant_files
    
    def _collect_descendant_ids(self, node: Dict[str, Any], all_nodes: List[Dict[str, Any]], visited: set) -> set:
        """ë…¸ë“œì˜ ëª¨ë“  í•˜ìœ„ ë…¸ë“œ IDë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        descendant_ids = set()
        
        # í˜„ì¬ ë…¸ë“œê°€ ì´ë¯¸ ë°©ë¬¸ëœ ê²½ìš° ë¬´í•œ ë£¨í”„ ë°©ì§€
        if node.get('id') in visited:
            return descendant_ids
        
        visited.add(node.get('id'))
        
        # ì§ì ‘ ìì‹ ë…¸ë“œë“¤ ì²˜ë¦¬
        for child_id in node.get('children_ids', []):
            child_node = next((n for n in all_nodes if n.get('id') == child_id), None)
            if child_node:
                # ìì‹ ë…¸ë“œ ID ì¶”ê°€
                descendant_ids.add(child_id)
                # ìì‹ ë…¸ë“œì˜ í•˜ìœ„ ë…¸ë“œë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘
                grandchildren_ids = self._collect_descendant_ids(child_node, all_nodes, visited.copy())
                descendant_ids.update(grandchildren_ids)
        
        return descendant_ids
    
