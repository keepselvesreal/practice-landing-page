# ìƒì„± ì‹œê°„: 2024-09-19 23:58:00 KST  
# í•µì‹¬ ë‚´ìš©: íŒŒì¼ ê¸°ë°˜ ì§€ì†ì„± Chapter ì»¨í…ìŠ¤íŠ¸ ìºì‹± ë§¤ë‹ˆì € (í”„ë¡œì„¸ìŠ¤ ê°„ ìºì‹œ ì§€ì†ì„±)
# ìƒì„¸ ë‚´ìš©:
#   - FileBasedChapterContextManager (ë¼ì¸ 20-80): íŒŒì¼ ê¸°ë°˜ ìºì‹œ ì§€ì†ì„± ë§¤ë‹ˆì €
#   - SessionInfoSerializer (ë¼ì¸ 85-140): SessionInfo ì§ë ¬í™”/ì—­ì§ë ¬í™”
#   - ChapterContextManager (ë¼ì¸ 145-220): íŒŒì¼ ìºì‹œ í†µí•© ì¥ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € 
#   - _save_cache_to_file (ë¼ì¸ 225-245): ìºì‹œ íŒŒì¼ ì €ì¥
#   - _load_cache_from_file (ë¼ì¸ 250-280): ìºì‹œ íŒŒì¼ ë¡œë“œ
# ìƒíƒœ: active
# ì°¸ì¡°: chapter_context_manager_v4.py (íŒŒì¼ ì§€ì†ì„± ì¶”ê°€)

import hashlib
import asyncio
import json
import pickle
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from pathlib import Path

from services.ai_service_v4 import SessionInfo

class SessionInfoSerializer:
    """SessionInfo ì§ë ¬í™”/ì—­ì§ë ¬í™” ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def serialize_session_info(session_info: SessionInfo) -> Dict[str, Any]:
        """SessionInfoë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì§ë ¬í™”"""
        try:
            # session_dataëŠ” ì œê³µìë³„ë¡œ ë‹¤ë¥¸ í˜•íƒœì´ë¯€ë¡œ pickleì„ ì‚¬ìš©í•˜ì—¬ ë°”ì´ë„ˆë¦¬ ì§ë ¬í™”
            serialized_session_data = pickle.dumps(session_info.session_data).hex()
            
            return {
                "provider_type": session_info.provider_type,
                "session_data_hex": serialized_session_data,
                "created_at": session_info.created_at,
                "message_count": session_info.message_count,
                "serialized_at": time.time()
            }
        except Exception as e:
            raise ValueError(f"SessionInfo ì§ë ¬í™” ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def deserialize_session_info(data: Dict[str, Any]) -> SessionInfo:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ SessionInfo ë³µì›"""
        try:
            # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ë³µì›
            session_data = pickle.loads(bytes.fromhex(data["session_data_hex"]))
            
            # SessionInfo ê°ì²´ ì¬ìƒì„±
            session_info = SessionInfo(
                provider_type=data["provider_type"],
                session_data=session_data
            )
            session_info.created_at = data["created_at"] 
            session_info.message_count = data["message_count"]
            
            return session_info
        except Exception as e:
            raise ValueError(f"SessionInfo ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")

class FileBasedChapterContextManager:
    """íŒŒì¼ ê¸°ë°˜ ìºì‹œ ì§€ì†ì„± ë§¤ë‹ˆì €"""
    
    def __init__(self, cache_dir: str = None):
        if cache_dir is None:
            # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
            cache_dir = Path(__file__).parent.parent.parent / "cache" / "chapter_contexts"
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì•¡ì„¸ìŠ¤ìš©)
        self.memory_cache: Dict[str, SessionInfo] = {}
        self.metadata_cache: Dict[str, dict] = {}
        
        # ì´ˆê¸°í™” ì‹œ íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ
        self._load_all_caches()
        
        self.cache_ttl = 3600  # 1ì‹œê°„ TTL
    
    def _get_cache_file_path(self, chapter_hash: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return self.cache_dir / f"cache_{chapter_hash}.json"
    
    def _save_cache_to_file(self, chapter_hash: str, session_info: SessionInfo, metadata: dict):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            cache_file = self._get_cache_file_path(chapter_hash)
            
            # SessionInfo ì§ë ¬í™”
            serialized_session = SessionInfoSerializer.serialize_session_info(session_info)
            
            cache_data = {
                "chapter_hash": chapter_hash,
                "session_info": serialized_session,
                "metadata": metadata,
                "saved_at": time.time()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ (í•´ì‹œ: {chapter_hash}): {e}")
    
    def _load_cache_from_file(self, chapter_hash: str) -> tuple[Optional[SessionInfo], Optional[dict]]:
        """íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
        try:
            cache_file = self._get_cache_file_path(chapter_hash)
            
            if not cache_file.exists():
                return None, None
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # TTL í™•ì¸
            saved_at = cache_data.get("saved_at", 0)
            if time.time() - saved_at > self.cache_ttl:
                # ë§Œë£Œëœ ìºì‹œ íŒŒì¼ ì‚­ì œ
                cache_file.unlink(missing_ok=True)
                return None, None
            
            # SessionInfo ì—­ì§ë ¬í™”
            session_info = SessionInfoSerializer.deserialize_session_info(cache_data["session_info"])
            metadata = cache_data["metadata"]
            
            return session_info, metadata
            
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (í•´ì‹œ: {chapter_hash}): {e}")
            return None, None
    
    def _load_all_caches(self):
        """ëª¨ë“  ìºì‹œ íŒŒì¼ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ"""
        if not self.cache_dir.exists():
            return
            
        cache_files = list(self.cache_dir.glob("cache_*.json"))
        loaded_count = 0
        
        for cache_file in cache_files:
            try:
                chapter_hash = cache_file.stem.replace("cache_", "")
                session_info, metadata = self._load_cache_from_file(chapter_hash)
                
                if session_info and metadata:
                    self.memory_cache[chapter_hash] = session_info
                    self.metadata_cache[chapter_hash] = metadata
                    loaded_count += 1
                    
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {cache_file} - {e}")
        
        if loaded_count > 0:
            print(f"ğŸ“‚ íŒŒì¼ì—ì„œ {loaded_count}ê°œ ìºì‹œ ë³µì› ì™„ë£Œ")
    
    def get_cache(self, chapter_hash: str) -> tuple[Optional[SessionInfo], Optional[dict]]:
        """ìºì‹œ ì¡°íšŒ (ë©”ëª¨ë¦¬ â†’ íŒŒì¼ ìˆœ)"""
        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if chapter_hash in self.memory_cache:
            return self.memory_cache[chapter_hash], self.metadata_cache.get(chapter_hash)
        
        # 2. íŒŒì¼ ìºì‹œ í™•ì¸
        session_info, metadata = self._load_cache_from_file(chapter_hash)
        if session_info and metadata:
            # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
            self.memory_cache[chapter_hash] = session_info
            self.metadata_cache[chapter_hash] = metadata
            
        return session_info, metadata
    
    def set_cache(self, chapter_hash: str, session_info: SessionInfo, metadata: dict):
        """ìºì‹œ ì €ì¥ (ë©”ëª¨ë¦¬ + íŒŒì¼)"""
        # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
        self.memory_cache[chapter_hash] = session_info
        self.metadata_cache[chapter_hash] = metadata
        
        # íŒŒì¼ ìºì‹œ ì €ì¥
        self._save_cache_to_file(chapter_hash, session_info, metadata)
    
    def update_metadata(self, chapter_hash: str, metadata_updates: dict):
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        if chapter_hash in self.metadata_cache:
            self.metadata_cache[chapter_hash].update(metadata_updates)
            
            # íŒŒì¼ì—ë„ ë°˜ì˜
            session_info = self.memory_cache.get(chapter_hash)
            if session_info:
                self._save_cache_to_file(chapter_hash, session_info, self.metadata_cache[chapter_hash])
    
    def cleanup_expired_caches(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        now = time.time()
        expired_hashes = []
        
        # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ë§Œë£Œëœ í•­ëª© ì°¾ê¸°
        for chapter_hash, metadata in self.metadata_cache.items():
            created_at = metadata.get("created_at", datetime.now()).timestamp() if isinstance(metadata.get("created_at"), datetime) else metadata.get("created_at", now)
            if now - created_at > self.cache_ttl:
                expired_hashes.append(chapter_hash)
        
        # ë§Œë£Œëœ ìºì‹œ ì œê±°
        for chapter_hash in expired_hashes:
            # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
            self.memory_cache.pop(chapter_hash, None)
            self.metadata_cache.pop(chapter_hash, None)
            
            # íŒŒì¼ì—ì„œ ì œê±°
            cache_file = self._get_cache_file_path(chapter_hash)
            cache_file.unlink(missing_ok=True)
        
        if expired_hashes:
            print(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ ì •ë¦¬: {len(expired_hashes)}ê°œ")
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        return {
            "memory_cache_size": len(self.memory_cache),
            "cache_usage": {hash_key: metadata.get("usage_count", 0) for hash_key, metadata in self.metadata_cache.items()},
            "cache_files": len(list(self.cache_dir.glob("cache_*.json")))
        }

class ChapterContextManager:
    """íŒŒì¼ ê¸°ë°˜ ìºì‹œë¥¼ í™œìš©í•œ ì¥ ì¡°í•©ë³„ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    
    def __init__(self, ai_service, logger, config_manager):
        self.ai_service = ai_service
        self.logger = logger
        self.config = config_manager
        
        # íŒŒì¼ ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì € ì‚¬ìš©
        self.file_cache = FileBasedChapterContextManager()

    def generate_chapter_hash(self, chapters: List[str]) -> str:
        """ì¥ ì¡°í•©ìœ¼ë¡œ ê³ ìœ  í•´ì‹œ ìƒì„±"""
        chapters_sorted = sorted(chapters)
        return hashlib.md5(':'.join(chapters_sorted).encode()).hexdigest()[:12]

    async def get_or_setup_context_cache(self, chapters: List[str], strategy):
        """ì¥ ì¡°í•©ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ í™•ë³´ + ì„¤ì • (íŒŒì¼ ìºì‹œ í™œìš©)"""
        chapter_hash = self.generate_chapter_hash(chapters)
        
        # ìºì‹œ ì¡°íšŒ (ë©”ëª¨ë¦¬ â†’ íŒŒì¼)
        session_info, metadata = self.file_cache.get_cache(chapter_hash)
        
        if session_info and metadata and await self._is_context_cache_valid(session_info):
            # ìºì‹œ ì¬ì‚¬ìš©
            self.logger.info(f"â™»ï¸ ì¥ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì¬ì‚¬ìš© (í•´ì‹œ: {chapter_hash}) | ì¥: {chapters}")
            created_time = metadata['created_at']
            if isinstance(created_time, datetime):
                time_str = created_time.strftime('%H:%M:%S')
            else:
                time_str = datetime.fromtimestamp(created_time).strftime('%H:%M:%S')
            self.logger.info(f"   ğŸ“Š ìºì‹œ ì •ë³´: ìƒì„±ì‹œê°„={time_str}, ì‚¬ìš©íšŸìˆ˜={metadata['usage_count']}")
            
            # ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
            metadata_updates = {
                "last_used": datetime.now(),
                "usage_count": metadata["usage_count"] + 1
            }
            self.file_cache.update_metadata(chapter_hash, metadata_updates)
            
            return session_info

        # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„± 
        session_info = await self.ai_service.create_session()
        
        # ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        await self._setup_chapter_context(session_info, strategy)
        
        # ìºì‹œì— ì €ì¥ (ë©”ëª¨ë¦¬ + íŒŒì¼)
        metadata = {
            "created_at": datetime.now(),
            "last_used": datetime.now(), 
            "chapters": chapters,
            "usage_count": 1
        }
        self.file_cache.set_cache(chapter_hash, session_info, metadata)
        
        self.logger.info(f"ğŸ†• ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„± (í•´ì‹œ: {chapter_hash}) | ì¥: {chapters}")
        cache_stats = self.file_cache.get_cache_stats()
        self.logger.info(f"   ğŸ’¾ ìºì‹œ í†µê³„: ë©”ëª¨ë¦¬ {cache_stats['memory_cache_size']}ê°œ, íŒŒì¼ {cache_stats['cache_files']}ê°œ")
        return session_info

    async def _setup_chapter_context(self, session_info, strategy):
        """ì»¨í…ìŠ¤íŠ¸ ìºì‹œì— ì¥ ë‚´ìš© ì„¤ì •"""
        # ì¥ content ë¡œë“œ
        contents = []
        book_path = Path(self.config.config.base_data_path) / strategy.book_name
        
        for chapter_id in strategy.target_chapters:
            chapter_content_file = book_path / chapter_id / "content.md"
            if chapter_content_file.exists():
                with open(chapter_content_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    contents.append(f"# {chapter_id}\n{content}")

        combined_content = "\n\n".join(contents)
        
        context_setup_prompt = f"""ë‹¤ìŒì€ ì°¸ì¡°í•  ì¥ë“¤ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´í›„ ì§ˆì˜ì—ì„œëŠ” ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

{combined_content}

**ì¤‘ìš”í•œ ì‘ë‹µ ê·œì¹™:**
1. ì´ ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ìˆë‹¤ê°€, ë‹¤ìŒ ì§ˆì˜ë“¤ì— ëŒ€í•´ ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
2. ì»¨í…ì¸ ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
3. **ë‹µë³€ ìƒì„± ì‹œ ë°˜ë“œì‹œ ì°¸ì¡°í•œ ì„¹ì…˜ëª…ê³¼ ì¤„ ë²ˆí˜¸ ì •ë³´ë¥¼ ë‹µë³€ ëì— í‘œì‹œí•´ì£¼ì„¸ìš”.**
   í˜•ì‹: [ì°¸ì¡°: ì„¹ì…˜ëª… (ì¤„ XX-XX)]"""

        await self.ai_service.query_with_persistent_session(context_setup_prompt, session_info)

    async def query_with_cached_context(self, query: str, session_info) -> str:
        """ìºì‹œëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ì§ˆì˜"""
        simple_query = f"ì§ˆì˜: {query}"
        
        self.logger.info(f"ğŸ¤– ìºì‹œëœ ì»¨í…ìŠ¤íŠ¸ë¡œ AI ì§ˆì˜ ì‹¤í–‰ ì¤‘...")
        self.logger.info(f"   ğŸ“ ì§ˆì˜ ê¸¸ì´: {len(simple_query)}ì")
        
        response = await self.ai_service.query_with_persistent_session(simple_query, session_info)
        
        self.logger.info(f"âœ… AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(response)}ì)")
        return response

    async def _is_context_cache_valid(self, session_info) -> bool:
        """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìœ íš¨ì„± ê²€ì¦"""
        try:
            return session_info is not None and hasattr(session_info, 'session_data')
        except:
            return False

    def cleanup_expired_caches(self):
        """ë§Œë£Œëœ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì •ë¦¬"""
        self.file_cache.cleanup_expired_caches()
        
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        return self.file_cache.get_cache_stats()